<!DOCTYPE html>
<html lang="en">

<head>
    <meta id="viewport" content="width=device-width, initial-scale=1.0">

    <link rel="stylesheet" href="socialicious/css/socialicious.css">
    <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/bootstrap/4.0.0-beta/css/bootstrap.min.css"
          integrity="sha384-/Y6pD6FV/Vv2HJnA6t+vslU6fwYXjCFtcEpHbNJ0lyAFsXTsjBbfaDjzALeQsN6M" crossorigin="anonymous">
    <link rel="stylesheet" href="styles.css">
    <meta id="keywords" content="Daniel Hershcovich, דניאל הרשקוביץ">
    <meta charset="UTF-8">
    <title>Daniel Hershcovich - Home Page</title>

    <!-- Global site tag (gtag.js) - Google Analytics -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=UA-101317305-2"></script>
    <script async defer src="https://buttons.github.io/buttons.js"></script>

</head>

<body>

<div class="cv container">
    <div id="top" class="top">
        <div class="row justify-content-xs-center">
            <div class="col">
                <div class="group container">
                    <div class="title">Daniel Hershcovich</div>
                    <div class="subtitle">דניאל הרשקוביץ</div>
                </div>
            </div>
            <div class="col avatar">
                <img class="img-responsive" src="index_files/daniel_hershcovich_2019.jpg" alt="Photo by Alana Ocano">
            </div>
        </div>
        <div class="row">
            <div class="col-xs-1 social">
                <a href="https://www.facebook.com/daniel.hershcovich" target="_blank"><i
                        class="icon-2x icon-facebook-sign"></i></a>
                <a href="http://il.linkedin.com/in/danielhershcovich" target="_blank"><i
                        class="icon-2x icon-linkedin-sign"></i></a>
                <a href="https://twitter.com/daniel_hers" target="_blank"><i class="icon-2x icon-twitter-sign"></i></a>
                <a href="https://goodreads.com/danielhers" target="_blank"><i
                        class="icon-2x icon-goodreads-sign"></i></a>
                <a href="https://github.com/danielhers" target="_blank"><i class="icon-2x icon-github-sign"></i></a>
                <a href="https://orcid.org/0000-0002-3966-8708" target="_blank" style="vertical-align:12px;"><img
                        src="index_files/ORCIDiD_icon32x32.png" alt="ORCID iD icon"></a>
            </div>
        </div>

        <hr>
    </div>
    <div class="row">
        <div id="accordion" class="col-xs-12">

            <div class="text" style="text-align:justify">
                <p>Tenure-Track Assistant Professor at <a href="http://coastalcph.github.io" title="NLP research group">CoAStaL</a>,
                    <a href="https://di.ku.dk/english/research/nlp/" target="_blank" title="NLP Section">Natural Language Processing section</a>,
                    <a href="https://di.ku.dk/english" target="_blank" title="Datalogisk Institut (Department of Computer Science)">Department of Computer Science</a>,
                    <a href="https://www.ku.dk/english" target="_blank">University of Copenhagen, Denmark</a>.</p>
                <p>Research interests:
                    <ul>
                        <li>Adapting and generalizing language models and data across cultures and languages.</li>
                        <li>Combining explicit representation of human values and knowledge into language models and their analysis.</li>
                        <li>Investigating language and identity with respect to food and promoting sustainable and diverse diets.</li>
                    </ul>
                </p>
            </div>

            <div class="menu"></div>

            <div class="card">
                <h2 class="header card-header" style="display: inline">
                    <button id="news" class="btn btn-link" data-toggle="collapse" data-target="#newsList" aria-expanded="true" aria-controls="newsList">News</button>
                </h2>
                <div id="newsList" class="contact collapse show" data-parent="#accordion">
                    <div class="news">
                        <ul class="card-body">
                          <li>I am co-organizing the <a href="https://sites.google.com/view/c3nlp">2nd Workshop on Cross-Cultural Considerations in NLP</a> at <a href="https://2024.aclweb.org/">ACL 2024</a>.</li>
			  <li>I am co-organizing the <a href="https://sites.google.com/view/c3nlp">2nd Workshop on Social Influence in Conversations</a> at <a href="https://2024.emnlp.org/">EMNLP 2024</a>.</li>
                          <li>I am program co-chair for <a href="https://www.nodalida-bhlt2025.eu/">NoDaLiDa/Baltic-HLT 2025</a>.</li>
			  <li>I am a faculty advisor to the Student Research Workshop chairs for <a href="https://2025.aclweb.org/">ACL 2025</a>.</li>
                        </ul>
                    </div>
                </div>
            </div>

            <div class="card">
                <h2 class="header card-header" style="display: inline">
                    <button id="projects" class="btn btn-link" data-toggle="collapse" data-target="#projectsList" aria-expanded="true" aria-controls="projectsList">Projects</button>
                </h2>
                <div id="projectsList" class="contact collapse show" data-parent="#accordion">
                    <div class="projects">
                        <ul class="card-body">
                          <li>Haptic Captioning: Using Natural Language Models to Design Haptic Experiences. <a href="https://veluxfoundations.dk/en/villum-experiment-2022">Villum Experiment</a> project with <a href="https://hastiseifi.com/">Hasti Seifi</a>.</li>
                          <li><a href="https://mime-memo.github.io/">Mining the Meaning</a>. <a href="https://saxo.ku.dk/forskning/support/Data__pulje_ans_gningsmateriale.pdf">Data+</a> project with <a href="https://nors.ku.dk/ansatte/?pure=da/persons/195540">Jens Bjerring-Hansen</a> (Department of Nordic Studies and Linguistics).</li>
                        </ul>
                    </div>
                </div>
            </div>

            <div class="card">
                <h2 class="header card-header" style="display: inline">
                    <button id="publications" class="btn btn-link" data-toggle="collapse" data-target="#publicationList" aria-expanded="true" aria-controls="publicationList">Publications</button>(<a href="http://scholar.google.com/citations?user=479qIucAAAAJ" target="_blank">Google Scholar</a>,
                    <a href="https://www.semanticscholar.org/author/Daniel-Hershcovich/2086349" target="_blank">Semantic Scholar</a>)
                </h2>

                <div id="publicationList" class="publications collapse show" data-parent="#accordion">
                    <ul class="card-body">
                      <h3 class="header">Peer-Reviewed Publications</h3>
                        <li><a id="unhappy"></a>
                            <div class="row">
                                <div class="col">
                                    <b><i>Unhappy Texts? A Gendered and Computational Rereading of The Modern Breakthrough</i></b>.
                                    <br>
                                    Kirstine Nielsen Degn, Jens Bjerring-Hansen, Ali Al-Laith and <b>Daniel Hershcovich</b>.
                                    <a href="https://www.jstor.org/journal/scanstud">Scandinavian Studies</a>, 2. 97, 2025 (Accepted/In press).
                                </div>
                                <div class="col btn-group mr-2 v-center" role="group">
                                    <a href="unhappy.bib" class="btn btn-outline-primary btn-sm bib">bib</a>
                                </div>
                            </div>
                        </li>

			<li><a id="amrfair"></a>
                            <div class="row">
                                <div class="col">
                                    <b><i>Can Abstract Meaning Representation Facilitate Fair Legal Judgement Predictions?</i></b>.
                                    <br>
                                    Supriti Vijay and <b>Daniel Hershcovich</b></b>.
                                    <a href="https://insights-workshop.github.io/2024/">Workshop on Insights from Negative Results in NLP 2024</a>.
                                </div>
                                <div class="col btn-group mr-2 v-center" role="group">
                                    <a href="https://aclanthology.org/2024.insights-1.13/" class="btn btn-outline-primary btn-sm">proceedings</a>
                                    <a href="#" class="btn btn-outline-primary btn-sm"
                                       onclick="$('#amrfair_abstract').slideToggle(); $(this).toggleClass('active'); return false;">abstract</a>
                                    <a href="amrfair.bib" class="btn btn-outline-primary btn-sm bib">bib</a>
                                </div>
                                <div id="amrfair_abstract" class="abstract row" style="display:none;">
                                  Legal judgment prediction encompasses the automated prediction of case outcomes by leveraging historical facts and opinions. While this approach holds the potential to enhance the efficiency of the legal system, it also raises critical concerns regarding the perpetuation of biases. Abstract Meaning Representation has shown promise as an intermediate text representation in various downstream NLP tasks due to its ability to capture semantically meaningful information in a graph-like structure. In this paper, we employ this ability of AMR in the legal judgement prediction task and assess to what extent it encodes biases, or conversely, abstracts away from them. Our study reveals that while AMR-based models exhibit worse overall performance than transformer-based models, they are less biased for attributes like age and defendant state compared to gender. By shedding light on these findings, this paper contributes to a more nuanced understanding of AMR’s potential benefits and limitations in legal NLP.
                                </div>
                            </div>
                        </li>

			<li><a id="anki"></a>
                            <div class="row">
                                <div class="col">
                                    <b><i>Automated Sentence Generation for a Spaced Repetition Software</i></b>.
                                    <br>
                                    Benjamin Paddags, <b>Daniel Hershcovich</b> and Valkyrie Arline Savage</b>.
                                    <a href="https://sig-edu.org/bea/2024">Workshop on Innovative Use of NLP for Building Educational Applications 2024</a>.
					<a href="https://drive.google.com/file/d/1tt6r1CtxfOREsq7rYecjfhwRN4uAcKGZ/view"><span style="color:red">Outstanding Paper Award</span></a>.
                                </div>
                                <div class="col btn-group mr-2 v-center" role="group">
                                    <a href="https://aclanthology.org/2024.bea-1.29/" class="btn btn-outline-primary btn-sm">proceedings</a>
                                    <a href="#" class="btn btn-outline-primary btn-sm"
                                       onclick="$('#anki_abstract').slideToggle(); $(this).toggleClass('active'); return false;">abstract</a>
                                    <a href="anki.bib" class="btn btn-outline-primary btn-sm bib">bib</a>
                                </div>
                                <div id="anki_abstract" class="abstract row" style="display:none;">
                                  This paper presents and tests AllAI, an app that utilizes state-of-the-art NLP technology to assist second language acquisition through a novel method of sentence-based spaced repetition. Diverging from current single word or fixed sentence repetition, AllAI dynamically combines words due for repetition into sentences, enabling learning words in context while scheduling them independently. This research explores various suitable NLP paradigms and finds a few-shot prompting approach and retrieval of existing sentences from a corpus to yield the best correctness and scheduling accuracy. Subsequently, it evaluates these methods on 26 learners of Danish, finding a four-fold increase in the speed at which new words are learned, compared to conventional spaced repetition. Users of the retrieval method also reported significantly higher enjoyment, hinting at a higher user engagement.
                                </div>
                            </div>
                        </li>
						
			  <li><a id="creoleval"></a>
				<div class="row">
				  <div class="col">
					<b><i>CreoleVal: Multilingual Multitask Benchmarks for Creoles</i></b>.
					<br>
					Heather Lent, Kushal Tatariya, Raj Dabre, Yiyi Chen, Marcell Fekete, Esther Ploeger, Li Zhou, Hans Erik Heje, Diptesh Kanojia, Paul Belony, Marcel Bollmann, Loïc Grobol, Miryam de Lhoneux, <b>Daniel Hershcovich</b>, Michel DeGraff, Anders Søgaard and Johannes Bjerva.
					TACL, 2024 (accepted).
				  </div>
				  <div class="col btn-group mr-2 v-center" role="group">
					<a href="https://arxiv.org/abs/2310.19567" class="btn btn-outline-primary btn-sm">preprint</a>
					<a href="#" class="btn btn-outline-primary btn-sm"
								onclick="$('#creoleval_abstract').slideToggle(); $(this).toggleClass('active'); return false;">abstract</a>
				  </div>
				  <div id="creoleval_abstract" class="abstract row" style="display:none;">
					<i>Creoles represent an under-explored and marginalized group of languages, with few available resources for NLP research. While the genealogical ties between Creoles and other highly-resourced languages imply a significant potential for transfer learning, this potential is hampered due to this lack of annotated data. In this work we present CreoleVal, a collection of benchmark datasets spanning 8 different NLP tasks, covering up to 28 Creole languages; it is an aggregate of brand new development datasets for machine comprehension, relation classification, and machine translation for Creoles, in addition to a practical gateway to a handful of preexisting benchmarks. For each benchmark, we conduct baseline experiments in a zero-shot setting in order to further ascertain the capabilities and limitations of transfer learning for Creoles. Ultimately, the goal of CreoleVal is to empower research on Creoles in NLP and computational linguistics. We hope this resource will contribute to technological inclusion for Creole language users around the globe.</i>
				  </div>
				</div>
			  </li>

                        <li><a id="globalaicultures2024"></a>
                            <div class="row">
                                <div class="col">
                                    <b><i>Vision-Language models under Cultural and Inclusive Considerations</i></b>.
                                    <br>
                                    Antonia Karamolegkou, Phillip Rust, Yong Cao, Ruixiang Cui, <b>Daniel Hershcovich</b> and Anders Søgaard.
                                    <a href="https://globalaicultures.github.io/">Workshop on Global AI Cultures 2024</a>.
                                </div>
                            </div>
                        </li>

                        <li><a id="memobert"></a>
                            <div class="row">
                                <div class="col">
                                    <b><i>Development and Evaluation of Pre-trained Language Models for Historical Danish and Norwegian Literary Texts</i></b>.
                                    <br>
                                    Ali Al-Laith, Alexander Conroy, Jens Bjerring-Hansen and <b>Daniel Hershcovich</b>.
                                    <a href="https://lrec-coling-2024.org">LREC-COLING 2024</a>.
                                </div>
                                <div class="col btn-group mr-2 v-center" role="group">
                                    <a href="https://aclanthology.org/2024.lrec-main.431/" class="btn btn-outline-primary btn-sm">proceedings</a>
                                    <a href="#" class="btn btn-outline-primary btn-sm"
                                       onclick="$('#memobert_abstract').slideToggle(); $(this).toggleClass('active'); return false;">abstract</a>
                                    <a href="memobert.bib" class="btn btn-outline-primary btn-sm bib">bib</a>
                                </div>
                                <div id="memobert_abstract" class="abstract row" style="display:none;">
                                  We develop and evaluate the first pre-trained language models specifically tailored for historical Danish and Norwegian texts. Three models are trained on a corpus of 19th-century Danish and Norwegian literature: two directly on the corpus with no prior pre-training, and one with continued pre-training. To evaluate the models, we utilize an existing sentiment classification dataset, and additionally introduce a new annotated word sense disambiguation dataset focusing on the concept of fate. Our assessment reveals that the model employing continued pre-training outperforms the others in two downstream NLP tasks on historical texts. Specifically, we observe substantial improvement in sentiment classification and word sense disambiguation compared to models trained on contemporary texts. These results highlight the effectiveness of continued pre-training for enhancing performance across various NLP tasks in historical text analysis.
                                </div>
                            </div>
                        </li>
                        
                        <li><a id="geoencoder"></a>
                            <div class="row">
                                <div class="col">
                                    <b><i>Geo-Encoder: A Chunk-Argument Bi-Encoder Framework for Chinese Geographic Re-Ranking</i></b>.
                                    <br>
                                    Yong Cao, Ruixue Ding, Boli Chen, Xianzhi Li, Min Chen, <b>Daniel Hershcovich</b>, Pengjun Xie and Fei Huang.
                                    <a href="https://2024.eacl.org/">EACL 2024</a>.
                                </div>
                                <div class="col btn-group mr-2 v-center" role="group">
                                    <a href="https://aclanthology.org/2024.eacl-long.91/" class="btn btn-outline-primary btn-sm">proceedings</a>
                                    <a href="https://arxiv.org/abs/2309.01606" class="btn btn-outline-primary btn-sm">preprint</a>
                                    <a href="#" class="btn btn-outline-primary btn-sm"
                                       onclick="$('#geoencoder_abstract').slideToggle(); $(this).toggleClass('active'); return false;">abstract</a>
                                    <a href="geoencoder.bib" class="btn btn-outline-primary btn-sm bib">bib</a>
                                </div>
                                <div id="geoencoder_abstract" class="abstract row" style="display:none;">
                                  Chinese geographic re-ranking task aims to find the most relevant addresses among retrieved candidates, which is crucial for location-related services such as navigation maps. Unlike the general sentences, geographic contexts are closely intertwined with geographical concepts, from general spans (e.g., province) to specific spans (e.g., road). Given this feature, we propose an innovative framework, namely Geo-Encoder, to more effectively integrate Chinese geographical semantics into re-ranking pipelines. Our methodology begins by employing off-the-shelf tools to associate text with geographical spans, treating them as chunking units. Then, we present a multi-task learning module to simultaneously acquire an effective attention matrix that determines chunk contributions to extra semantic representations. Furthermore, we put forth an asynchronous update mechanism for the proposed addition task, aiming to guide the model capable of effectively focusing on specific chunks. Experiments on two distinct Chinese geographic re-ranking datasets, show that the Geo-Encoder achieves significant improvements when compared to state-of-the-art baselines. Notably, it leads to a substantial improvement in the Hit@1 score of MGEO-BERT, increasing it by 6.22% from 62.76 to 68.98 on the GeoTES dataset.
                                </div>
                            </div>
                        </li>
                        
                        <li><a id="cudialog"></a>
                            <div class="row">
                                <div class="col">
                                    <b><i>Bridging Cultural Nuances in Dialogue Agents through Cultural Value Surveys</i></b>.
                                    <br>
                                    Yong Cao, Min Chen, <b>Daniel Hershcovich</b>.
                                    Findings of <a href="https://2024.eacl.org/">EACL 2024</a>.
                                </div>
                                <div class="col btn-group mr-2 v-center" role="group">
                                    <a href="https://aclanthology.org/2024.findings-eacl.63/" class="btn btn-outline-primary btn-sm">proceedings</a>
                                    <a href="http://arxiv.org/abs/2401.10352" class="btn btn-outline-primary btn-sm">preprint</a>
                                    <a href="#" class="btn btn-outline-primary btn-sm"
                                       onclick="$('#cudialog_abstract').slideToggle(); $(this).toggleClass('active'); return false;">abstract</a>
                                    <a href="cudialog.bib" class="btn btn-outline-primary btn-sm bib">bib</a>
                                </div>
                                <div id="cudialog_abstract" class="abstract row" style="display:none;">
                                  The cultural landscape of interactions with dialogue agents is a compelling yet relatively unexplored territory. It's clear that various sociocultural aspects -- from communication styles and beliefs to shared metaphors and knowledge -- profoundly impact these interactions. To delve deeper into this dynamic, we introduce cuDialog, a first-of-its-kind benchmark for dialogue generation with a cultural lens. We also develop baseline models capable of extracting cultural attributes from dialogue exchanges, with the goal of enhancing the predictive accuracy and quality of dialogue agents. To effectively co-learn cultural understanding and multi-turn dialogue predictions, we propose to incorporate cultural dimensions with dialogue encoding features. Our experimental findings highlight that incorporating cultural value surveys boosts alignment with references and cultural markers, demonstrating its considerable influence on personalization and dialogue quality. To facilitate further exploration in this exciting domain, we publish our benchmark publicly accessible at https://github.com/yongcaoplus/cuDialog.
                                </div>
                            </div>
                        </li>
                        
                        <li><a id="recipes"></a>
                            <div class="row">
                                <div class="col">
                                    <b><i>Cultural Adaptation of Recipes</i></b>.
                                    <br>
                                    Yong Cao, Yova Kementchedjhieva, Ruixiang Cui, Antonia Karamolegkou, Li Zhou, Megan Dare, Lucia Donatelli and <b>Daniel Hershcovich</b>.
                                    <a href="https://transacl.org/">TACL</a>, 2023.
                                </div>
                                <div class="col btn-group mr-2 v-center" role="group">
				    <a href="https://aclanthology.org/2024.tacl-1.5/" class="btn btn-outline-primary btn-sm">proceedings</a>
                                    <a href="https://direct.mit.edu/tacl/article/doi/10.1162/tacl_a_00634/119279/Cultural-Adaptation-of-Recipes" class="btn btn-outline-primary btn-sm">MIT Press</a>
                                    <a href="https://arxiv.org/abs/2310.17353" class="btn btn-outline-primary btn-sm">preprint</a>
                                    <a href="#" class="btn btn-outline-primary btn-sm"
                                       onclick="$('#recipes_abstract').slideToggle(); $(this).toggleClass('active'); return false;">abstract</a>
                                    <a href="recipes.bib" class="btn btn-outline-primary btn-sm bib">bib</a>
                                </div>
                                <div id="recipes_abstract" class="abstract row" style="display:none;">
                                  Building upon the considerable advances in Large Language Models (LLMs), we are now equipped to address more sophisticated tasks demanding a nuanced understanding of cross-cultural contexts. A key example is recipe adaptation, which goes beyond simple translation to include a grasp of ingredients, culinary techniques, and dietary preferences specific to a given culture. We introduce a new task involving the translation and cultural adaptation of recipes between Chinese and English-speaking cuisines. To support this investigation, we present CulturalRecipes, a unique dataset comprised of automatically paired recipes written in Mandarin Chinese and English. This dataset is further enriched with a human-written and curated test set. In this intricate task of cross-cultural recipe adaptation, we evaluate the performance of various methods, including GPT-4 and other LLMs, traditional machine translation, and information retrieval techniques. Our comprehensive analysis includes both automatic and human evaluation metrics. While GPT-4 exhibits impressive abilities in adapting Chinese recipes into English, it still lags behind human expertise when translating English recipes into Chinese. This underscores the multifaceted nature of cultural adaptations. We anticipate that these insights will significantly contribute to future research on culturally-aware language models and their practical application in culturally diverse contexts.
                                </div>
                            </div>
                        </li>
                        
                        <li><a id="compass"></a>
                            <div class="row">
                                <div class="col">
                                    <b><i>Cultural Compass: Predicting Transfer Learning Success in Offensive Language Detection with Cultural Features</i></b>.
                                    <br>
                                    Li Zhou, Antonia Karamolegkou, Wenyu Chen and <b>Daniel Hershcovich</b>.
                                    Findings of <a href="https://2023.emnlp.org/">EMNLP 2023</a>.
                                </div>
                                <div class="col btn-group mr-2 v-center" role="group">
                                    <a href="https://aclanthology.org/2023.findings-emnlp.845/" class="btn btn-outline-primary btn-sm">proceedings</a>
                                    <a href="https://arxiv.org/abs/2310.06458" class="btn btn-outline-primary btn-sm">preprint</a>
                                    <a href="#" class="btn btn-outline-primary btn-sm"
                                       onclick="$('#compass_abstract').slideToggle(); $(this).toggleClass('active'); return false;">abstract</a>
                                    <a href="compass.bib" class="btn btn-outline-primary btn-sm bib">bib</a>
                                </div>
                                <div id="compass_abstract" class="abstract row" style="display:none;">
                                  The increasing ubiquity of language technology necessitates a shift towards considering cultural diversity in the machine learning realm, particularly for subjective tasks that rely heavily on cultural nuances, such as Offensive Language Detection (OLD). Current understanding underscores that these tasks are substantially influenced by cultural values, however, a notable gap exists in determining if cultural features can accurately predict the success of cross-cultural transfer learning for such subjective tasks. Addressing this, our study delves into the intersection of cultural features and transfer learning effectiveness. The findings reveal that cultural value surveys indeed possess a predictive power for cross-cultural transfer learning success in OLD tasks and that it can be further improved using offensive word distance. Based on these results, we advocate for the integration of cultural information into datasets. Additionally, we recommend leveraging data sources rich in cultural information, such as surveys, to enhance cultural adaptability. Our research signifies a step forward in the quest for more inclusive, culturally sensitive language technologies.
                                </div>
                            </div>
                        </li>
                        
                        <li><a id="hyperprobe"></a>
                            <div class="row">
                                <div class="col">
                                    <b><i>Probing for Hyperbole in Pre-Trained Language Models</i></b>.
                                    <br>
                                    Nina Skovgaard Schneidermann, <b>Daniel Hershcovich</b> and Bolette Sandford Pedersen.
                                    <a href="https://acl2023-srw.github.io/">ACL Student Research Workshop (SRW) 2023</a>.
                                </div>
                                <div class="col btn-group mr-2 v-center" role="group">
                                    <a href="https://aclanthology.org/2023.acl-srw.30/" class="btn btn-outline-primary btn-sm">proceedings</a>
                                    <a href="hyperprobe.pdf" class="btn btn-outline-primary btn-sm">pdf</a>
                                    <a href="#" class="btn btn-outline-primary btn-sm"
                                       onclick="$('#hyperprobe_abstract').slideToggle(); $(this).toggleClass('active'); return false;">abstract</a>
                                    <a href="hyperprobe.bib" class="btn btn-outline-primary btn-sm bib">bib</a>
                                </div>
                                <div id="hyperprobe_abstract" class="abstract row" style="display:none;">
                                    Hyperbole is a common figure of speech, which is under-explored in NLP research. In this study, we conduct edge and minimal description length (MDL) probing experiments for three pre-trained language models (PLMs) in an attempt to explore the extent to which hyperbolic information is encoded in these models. We use both word-in-context and sentencelevel representations as model inputs as a basis for comparison. We also annotate 63 hyperbole sentences from the HYPO dataset according to an operational taxonomy to conduct an error analysis to explore the encoding of different hyperbole categories. Our results show that hyperbole is to a limited extent encoded in PLMs, and mostly in the final layers. They also indicate that hyperbolic information may be better encoded by the sentence-level representations, which, due to the pragmatic nature of hyperbole, may therefore provide a more accurate and informative representation in PLMs. Finally, the inter-annotator agreement for our annotations, a Cohen’s Kappa of 0.339, suggest that the taxonomy categories may not be intuitive and need revision or simplification.
                                </div>
                            </div>
                        </li>
                        
                        <li><a id="mcwqr"></a>
                            <div class="row">
                                <div class="col">
                                    <b><i>On Evaluating Multilingual Compositional Generalization with Translated Datasets</i></b>.
                                    <br>
                                    Zi Wang and <b>Daniel Hershcovich</b>.
                                    <a href="https://2023.aclweb.org/">ACL 2023</a>.
                                </div>
                                <div class="col btn-group mr-2 v-center" role="group">
                                    <a href="https://aclanthology.org/2023.acl-long.93/" class="btn btn-outline-primary btn-sm">proceedings</a>
                                    <a href="https://arxiv.org/abs/2306.11420" class="btn btn-outline-primary btn-sm">preprint</a>
                                    <a href="mcwqr.pdf" class="btn btn-outline-primary btn-sm">pdf</a>
                                    <a href="#" class="btn btn-outline-primary btn-sm"
                                       onclick="$('#mcwqr_abstract').slideToggle(); $(this).toggleClass('active'); return false;">abstract</a>
                                    <a href="mcwqr.bib" class="btn btn-outline-primary btn-sm bib">bib</a>
                                </div>
                                <div id="mcwqr_abstract" class="abstract row" style="display:none;">
                                    Compositional generalization allows efficient learning and human-like inductive biases. Since most research investigating compositional generalization in NLP is done on English, important questions remain underexplored. Do the necessary compositional generalization abilities differ across languages? Can models compositionally generalize cross-lingually? As a first step to answering these questions, recent work used neural machine translation to translate datasets for evaluating compositional generalization in semantic parsing. However, we show that this entails critical semantic distortion. To address this limitation, we craft a faithful rule-based translation of the MCWQ dataset from English to Chinese and Japanese. Even with the resulting robust benchmark, which we call MCWQ-R, we show that the distribution of compositions still suffers due to linguistic divergences, and that multilingual models still struggle with cross-lingual compositional generalization. Our dataset and methodology will be useful resources for the study of cross-lingual compositional generalization in other tasks.
                                </div>
                            </div>
                        </li>
                       
                        <li><a id="respectively"></a>
                            <div class="row">
                                <div class="col">
                                    <b><i>What does the Failure to Reason with "Respectively'' in Zero/Few-Shot Settings Tell Us about Language Models?</i></b>.
                                    <br>
                                    Ruixiang Cui, Seolhwa Lee, <b>Daniel Hershcovich</b> and Anders Søgaard.
                                    <a href="https://2023.aclweb.org/">ACL 2023</a>.
                                </div>
                                <div class="col btn-group mr-2 v-center" role="group">
                                    <a href="https://aclanthology.org/2023.acl-long.489/" class="btn btn-outline-primary btn-sm">proceedings</a>
                                    <a href="https://arxiv.org/abs/2305.19597" class="btn btn-outline-primary btn-sm">preprint</a>
                                    <a href="#" class="btn btn-outline-primary btn-sm"
                                       onclick="$('#respectively_abstract').slideToggle(); $(this).toggleClass('active'); return false;">abstract</a>
                                    <a href="respectively.bib" class="btn btn-outline-primary btn-sm bib">bib</a>
                                </div>
                                <div id="respectively_abstract" class="abstract row" style="display:none;">
                                    Humans can effortlessly understand the coordinate structure of sentences such as "Niels Bohr and Kurt Cobain were born in Copenhagen and Seattle, respectively". In the context of natural language inference (NLI), we examine how language models (LMs) reason with respective readings (Gawron and Kehler, 2004) from two perspectives: syntactic-semantic and commonsense-world knowledge. We propose a controlled synthetic dataset WikiResNLI and a naturally occurring dataset NatResNLI to encompass various explicit and implicit realizations of "respectively". We show that fine-tuned NLI models struggle with understanding such readings without explicit supervision. While few-shot learning is easy in the presence of explicit cues, longer training is required when the reading is evoked implicitly, leaving models to rely on common sense inferences. Furthermore, our fine-grained analysis indicates models fail to generalize across different constructions. To conclude, we demonstrate that LMs still lag behind humans in generalizing to the long tail of linguistic constructions.
                                </div>
                            </div>
                        </li>
                        
                        <li><a id="superhuman"></a>
                            <div class="row">
                                <div class="col">
                                    <b><i>What's the Meaning of Superhuman Performance in Today's NLU?</i></b>.
                                    <br>
                                    Simone Tedeschi, Johan Bos, Thierry Declerck, Jan Hajic, <b>Daniel Hershcovich</b>, Eduard H. Hovy, Alexander Koller, Simon Krek, Steven Schockaert, Rico Sennrich, Ekaterina Shutova and Roberto Navigli.
                                    <a href="https://2023.aclweb.org/">ACL 2023</a>.
                                    <a href='https://2023.aclweb.org/program/best_papers/'
                                       target="_blank"><span style="color:red">Outstanding Paper Award</span></a>.
                                </div>
                                <div class="col btn-group mr-2 v-center" role="group">
                                    <a href="https://aclanthology.org/2023.acl-long.697/" class="btn btn-outline-primary btn-sm">proceedings</a>
                                    <a href="https://arxiv.org/abs/2305.08414" class="btn btn-outline-primary btn-sm">preprint</a>
                                    <a href="#" class="btn btn-outline-primary btn-sm"
                                       onclick="$('#superhuman_abstract').slideToggle(); $(this).toggleClass('active'); return false;">abstract</a>
                                    <a href="superhuman.bib" class="btn btn-outline-primary btn-sm bib">bib</a>
                                </div>
                                <div id="superhuman_abstract" class="abstract row" style="display:none;">
                                  In the last five years, there has been a significant focus in Natural Language Processing (NLP) on developing larger Pretrained Language Models (PLMs) and introducing benchmarks such as SuperGLUE and SQuAD to measure their abilities in language understanding, reasoning, and reading comprehension. These PLMs have achieved impressive results on these benchmarks, even surpassing human performance in some cases. This has led to claims of superhuman capabilities and the provocative idea that certain tasks have been solved. In this position paper, we take a critical look at these claims and ask whether PLMs truly have superhuman abilities and what the current benchmarks are really evaluating. We show that these benchmarks have serious limitations affecting the comparison between humans and PLMs and provide recommendations for fairer and more transparent benchmarks.
                                </div>
                            </div>
               
                        </li>
                        
                        <li><a id="kbqa_relation"></a>
                            <div class="row">
                                <div class="col">
                                    <b><i>Pay More Attention to Relation Exploration for Knowledge Base Question Answering</i></b>.
                                    <br>
                                    Yong Cao, Xianzhi Li, huiwen liu, Wen Dai, shuai chen, Bin Wang, Min Chen and <b>Daniel Hershcovich</b>.
                                    Findings of <a href="https://2023.aclweb.org/">ACL 2023</a>.
                                </div>
                                <div class="col btn-group mr-2 v-center" role="group">
                                    <a href="https://aclanthology.org/2023.findings-acl.133/" class="btn btn-outline-primary btn-sm">proceedings</a>
                                    <a href="https://arxiv.org/abs/2305.02118" class="btn btn-outline-primary btn-sm">preprint</a>
                                    <a href="#" class="btn btn-outline-primary btn-sm"
                                       onclick="$('#kbqa_relation_abstract').slideToggle(); $(this).toggleClass('active'); return false;">abstract</a>
                                    <a href="kbqa_relation.bib" class="btn btn-outline-primary btn-sm bib">bib</a>
                                </div>
                                <div id="kbqa_relation_abstract" class="abstract row" style="display:none;">
                                  Knowledge base question answering (KBQA) is a challenging task that aims to retrieve correct answers from large-scale knowledge bases. Existing attempts primarily focus on entity representation and final answer reasoning, which results in limited supervision for this task. Moreover, the relations, which empirically determine the reasoning path selection, are not fully considered in recent advancements. In this study, we propose a novel framework, RE-KBQA, that utilizes relations in the knowledge base to enhance entity representation and introduce additional supervision. We explore guidance from relations in three aspects, including (1) distinguishing similar entities by employing a variational graph auto-encoder to learn relation importance; (2) exploring extra supervision by predicting relation distributions as soft labels with a multi-task scheme; (3) designing a relation-guided re-ranking algorithm for post-processing. Experimental results on two benchmark datasets demonstrate the effectiveness and superiority of our framework, improving the F1 score by 5.7% from 40.5 to 46.3 on CWQ and 5.8% from 62.8 to 68.5 on WebQSP, better or on par with state-of-the-art methods.</div>
                            </div>
                        </li>
                        
                        <li><a id="nodalida2023"></a>
                            <div class="row">
                                <div class="col">
                                    <b><i>Sentiment Classification of Historical Danish and Norwegian Literary Texts</i></b>.
                                    <br>
                                    Ali Al-Laith, Kirstine Nielsen Degn, Alexander Conroy, Bolette S. Pedersen, Jens Bjerring-Hansen and <b>Daniel Hershcovich</b>.
                                    <a href="https://www.nodalida2023.fo/">NoDaLiDa 2023</a>.
                                </div>
                                <div class="col btn-group mr-2 v-center" role="group">
                                    <a href="https://aclanthology.org/2023.nodalida-1.34/" class="btn btn-outline-primary btn-sm">proceedings</a>
                                    <a href="https://openreview.net/pdf?id=dszKbb2GH3" class="btn btn-outline-primary btn-sm">preprint</a>
                                    <a href="#" class="btn btn-outline-primary btn-sm"
                                       onclick="$('#nodalida2023_abstract').slideToggle(); $(this).toggleClass('active'); return false;">abstract</a>
                                    <a href="nodalida2023.bib" class="btn btn-outline-primary btn-sm bib">bib</a>
                                </div>
                                <div id="nodalida2023_abstract" class="abstract row" style="display:none;">
                                  Sentiment classification is valuable for literary analysis, as sentiment is crucial in literary narratives. It can, for example, be used to investigate a hypothesis in the literary analysis of 19th-century Scandinavian novels that the writing of female authors in this period was characterized by negative sentiment, as this paper shows. In order to enable a data-driven analysis of this hypothesis, we create a manually annotated dataset of sentence-level sentiment annotations for novels from this period and use it to train and evaluate various sentiment classification methods. We find that pre-trained multilingual language models outperform models trained on modern Danish, as well as classifiers based on lexical resources. Finally, in classifier-assisted corpus analysis, we confirm the literary hypothesis regarding the author's gender and further shed light on the temporal development of the trend. Our dataset and trained models will be useful for future analysis of historical  Danish and Norwegian literary texts.
                                </div>
                            </div>
                        </li>
                        
                        <li><a id="c3nlp_offensive"></a>
                            <div class="row">
                                <div class="col">
                                    <b><i>Cross-Cultural Transfer Learning for Chinese Offensive Language Detection</i></b>.
                                    <br>
                                    Li Zhou, Laura Cabello, Yong Cao and <b>Daniel Hershcovich</b>.
                                    <a href="https://sites.google.com/view/c3nlp/home">C3NLP 2023</a>.
                                </div>
                                <div class="col btn-group mr-2 v-center" role="group">
                                    <a href="https://aclanthology.org/2023.c3nlp-1.2/" class="btn btn-outline-primary btn-sm">proceedings</a>
                                    <a href="https://arxiv.org/abs/2303.17927" class="btn btn-outline-primary btn-sm">preprint</a>
                                    <a href="#" class="btn btn-outline-primary btn-sm"
                                       onclick="$('#c3nlp_offensive_abstract').slideToggle(); $(this).toggleClass('active'); return false;">abstract</a>
                                    <a href="2023.c3nlp-1.2.bib" class="btn btn-outline-primary btn-sm bib">bib</a>
                                </div>
                                <div id="c3nlp_offensive_abstract" class="abstract row" style="display:none;">
                                  Detecting offensive language is a challenging task. Generalizing across different cultures and languages becomes even more challenging: besides lexical, syntactic and semantic differences, pragmatic aspects such as cultural norms and sensitivities, which are particularly relevant in this context, vary greatly. In this paper, we target Chinese offensive language detection and aim to investigate the impact of transfer learning using offensive language detection data from different cultural backgrounds, specifically Korean and English. We find that culture-specific biases in what is considered offensive negatively impact the transferability of language models (LMs) and that LMs trained on diverse cultural data are sensitive to different features in Chinese offensive language detection. In a few-shot learning scenario, however, our study shows promising prospects for non-English offensive language detection with limited resources. Our findings highlight the importance of cross-cultural transfer learning in improving offensive language detection and promoting inclusive digital spaces.
                                </div>
                            </div>
                        </li>

                        <li><a id="c3nlp_values"></a>
                            <div class="row">
                                <div class="col">
                                    <b><i>Assessing Cross-Cultural Alignment between ChatGPT and Human Societies: An Empirical Study</i></b>.
                                    <br>
                                    Yong Cao, Li Zhou, Seolhwa Lee, Laura Cabello, Min Chen and <b>Daniel Hershcovich</b>.
                                    <a href="https://sites.google.com/view/c3nlp/home">C3NLP 2023</a>.
                                </div>
                                <div class="col btn-group mr-2 v-center" role="group">
                                    <a href="https://aclanthology.org/2023.c3nlp-1.7/" class="btn btn-outline-primary btn-sm">proceedings</a>
                                    <a href="https://arxiv.org/abs/2303.17466" class="btn btn-outline-primary btn-sm">preprint</a>
                                    <a href="#" class="btn btn-outline-primary btn-sm"
                                       onclick="$('#c3nlp_values_abstract').slideToggle(); $(this).toggleClass('active'); return false;">abstract</a>
                                    <a href="2023.c3nlp-1.7.bib" class="btn btn-outline-primary btn-sm bib">bib</a>
                                </div>
                                <div id="c3nlp_values_abstract" class="abstract row" style="display:none;">
                                  The recent release of ChatGPT has garnered widespread recognition for its exceptional ability to generate human-like responses in dialogue. Given its usage by users from various nations and its training on a vast multilingual corpus that incorporates diverse cultural and societal norms, it is crucial to evaluate its effectiveness in cultural adaptation. In this paper, we investigate the underlying cultural background of ChatGPT by analyzing its responses to questions designed to quantify human cultural differences. Our findings suggest that, when prompted with American context, ChatGPT exhibits a strong alignment with American culture, but it adapts less effectively to other cultural contexts. Furthermore, by using different prompts to probe the model, we show that English prompts reduce the variance in model responses, flattening out cultural differences and biasing them towards American culture. This study provides valuable insights into the cultural implications of ChatGPT and highlights the necessity of greater diversity and cultural awareness in language technologies.
                                </div>
                            </div>
                        </li>

                        <li><a id="preregistration"></a>
                            <div class="row">
                                <div class="col">
                                    <b><i>A Two-Sided Discussion of Preregistration of NLP Research</i></b>.
                                    <br>
                                    Anders Søgaard, <b>Daniel Hershcovich</b> and Miryam de Lhoneux.
                                    <a href="https://2023.eacl.org/">EACL 2023</a>.
                                </div>
                                <div class="col btn-group mr-2 v-center" role="group">
                                    <a href="https://aclanthology.org/2023.eacl-main.6/" class="btn btn-outline-primary btn-sm">proceedings</a>
                                    <a href="preregistration.pdf" class="btn btn-outline-primary btn-sm">pdf</a>
                                    <a href="https://arxiv.org/abs/2302.10086" class="btn btn-outline-primary btn-sm">preprint</a>
                                    <a href="#" class="btn btn-outline-primary btn-sm"
                                       onclick="$('#preregistration_abstract').slideToggle(); $(this).toggleClass('active'); return false;">abstract</a>
                                    <a href="preregistration.bib" class="btn btn-outline-primary btn-sm bib">bib</a>
                                    <a href="eacl2023_preregistration_slides.pdf" class="btn btn-outline-primary btn-sm">slides</a>
                                    <a href="eacl2023_preregistration_poster.pdf" class="btn btn-outline-primary btn-sm">poster</a>
                                </div>
                                <div id="preregistration_abstract" class="abstract row" style="display:none;">
                                  Van Miltenburg et al. (2021) suggest NLP re- search should adopt preregistration to prevent fishing expeditions and to promote publication of negative results. At face value, this is a very reasonable suggestion, seemingly solving many methodological problems with NLP re- search. We discuss pros and cons—some old, some new: a) Preregistration is challenged by the practice of retrieving hypotheses after the results are known; b) preregistration may bias NLP toward confirmatory research; c) prereg- istration must allow for reclassification of re- search as exploratory; d) preregistration may in- crease publication bias; e) preregistration may increase flag-planting; f) preregistration may increase p-hacking; and finally, g) preregistra- tion may make us less risk tolerant. We cast our discussion as a dialogue, presenting both sides of the debate.
                                </div>
                            </div>
                        </li>

                        <li><a id="sustainabletweets"></a>
                            <div class="row">
                                <div class="col">
                                    <b><i>A Dataset of Sustainable Diet Arguments on Twitter</i></b>.
                                    <br>
                                    Marcus Astrup Hansen and <b>Daniel Hershcovich</b>.
                                    <a href="https://sites.google.com/view/nlp4positiveimpact">Workshop on NLP for Positive Impact 2022</a>.
                                </div>
                                <div class="col btn-group mr-2 v-center" role="group">
                                    <a href="https://aclanthology.org/2022.nlp4pi-1.5/" class="btn btn-outline-primary btn-sm">proceedings</a>
                                    <a href="sustainabletweets.pdf" class="btn btn-outline-primary btn-sm">pdf</a>
                                    <a href="#" class="btn btn-outline-primary btn-sm"
                                       onclick="$('#sustainabletweets_abstract').slideToggle(); $(this).toggleClass('active'); return false;">abstract</a>
                                    <a href="2022.nlp4pi-1.5.bib" class="btn btn-outline-primary btn-sm bib">bib</a>
                                    <a href="nlp4pi2022-sustainabletweets-slides.pdf" class="btn btn-outline-primary btn-sm">slides</a>
                                    <a href="nlp4pi2022_sustainabletweets_poster.pdf" class="btn btn-outline-primary btn-sm">poster</a>
                                    <a href="https://github.com/danielhers/sustainable-diet-arguments-twitter"
                                       class="btn btn-outline-primary btn-sm">data</a>
                                </div>
                                <div id="sustainabletweets_abstract" class="abstract row" style="display:none;">
                                  Sustainable development requires a significant change in our dietary habits. Argument mining can help achieve this goal by both affecting and helping understand people's behavior. We design an annotation scheme for argument mining from online discourse around sustainable diets, including novel evidence types specific to this domain. Using Twitter as a source, we crowdsource a dataset of 597 tweets annotated in relation to 5 topics. We benchmark a variety of NLP models on this dataset, demonstrating strong performance in some sub-tasks, while highlighting remaining challenges.
                                </div>
                            </div>
                        </li>

                        <li><a id="amrlogic"></a>
                            <div class="row">
                                <div class="col">
                                    <b><i>Can AMR Assist Legal and Logical Reasoning?</i></b>.
                                    <br>
                                    Nikolaus Schrack, Ruixiang Cui, Hugo A. López and <b>Daniel Hershcovich</b>.
                                    Findings of <a href="https://2022.emnlp.org/">EMNLP 2022</a> (long paper).
                                </div>
                                <div class="col btn-group mr-2 v-center" role="group">
                                    <a href="https://aclanthology.org/2022.findings-emnlp.112/" class="btn btn-outline-primary btn-sm">proceedings</a>
                                    <a href="amrlogic.pdf" class="btn btn-outline-primary btn-sm">pdf</a>
                                    <a href="#" class="btn btn-outline-primary btn-sm"
                                       onclick="$('#amrlogic_abstract').slideToggle(); $(this).toggleClass('active'); return false;">abstract</a>
                                    <a href="2022.findings-emnlp.112.bib" class="btn btn-outline-primary btn-sm bib">bib</a>
                                    <a href="https://github.com/nschrack/fusion" class="btn btn-outline-primary btn-sm">code</a>
                                    <a href="amrlogic_slides.pdf" class="btn btn-outline-primary btn-sm">slides</a>
                                    <a href="amrlogic_poster.pdf" class="btn btn-outline-primary btn-sm">poster</a>
                                </div>
                                <div id="amrlogic_abstract" class="abstract row" style="display:none;">
                                  Abstract Meaning Representation (AMR) has been shown to be useful for many downstream tasks. In this work, we explore the use of AMR for legal and logical reasoning, proposing neural architectures that utilize linearised AMR graphs in combination with pre-trained language models. While these models are not able to outperform text-only baselines, they correctly solve different instances than the text models, suggesting complementary abilities. Error analysis further reveals that AMR parsing quality is the most prominent challenge, especially regarding inputs with multiple sentences. We conduct a theoretical analysis of how logical relations are represented in AMR and conclude it might be helpful in some logical statements but not for others.
                                </div>
                            </div>
                        </li>

                        <li><a id="green"></a>
                            <div class="row">
                                <div class="col">
                                    <b><i>Towards Climate Awareness in NLP Research</i></b>.
                                    <br>
                                    <b>Daniel Hershcovich</b>, Nicolas Webersinke, Mathias Kraus, Julia Anna Bingler and Markus Leippold.
                                    <a href="https://2022.emnlp.org/">EMNLP 2022</a> (long paper).
                                </div>
                                <div class="col btn-group mr-2 v-center" role="group">
                                    <a href="https://aclanthology.org/2022.emnlp-main.159/" class="btn btn-outline-primary btn-sm">proceedings</a>
                                    <a href="green.pdf" class="btn btn-outline-primary btn-sm">pdf</a>
                                    <a href="https://arxiv.org/abs/2205.05071" class="btn btn-outline-primary btn-sm">preprint</a>
                                    <a href="#" class="btn btn-outline-primary btn-sm"
                                       onclick="$('#green_abstract').slideToggle(); $(this).toggleClass('active'); return false;">abstract</a>
                                    <a href="2022.emnlp-main.159.bib" class="btn btn-outline-primary btn-sm bib">bib</a>
                                    <a href="https://github.com/danielhers/climate-awareness-nlp"
                                       class="btn btn-outline-primary btn-sm">code</a>
                                    <a href="emnlp2022-climate-awareness-nlp-slides.pdf" class="btn btn-outline-primary btn-sm">slides</a>
                                </div>
                                <div id="green_abstract" class="abstract row" style="display:none;">
                                    The climate impact of AI, and NLP research in particular, has become a serious issue given the enormous amount of energy that is increasingly being used for training and running computational models. Consequently, increasing focus is placed on efficient NLP. However, this important initiative lacks simple guidelines that would allow for systematic climate reporting of NLP research. We argue that this deficiency is one of the reasons why very few publications in NLP report key figures that would allow a more thorough examination of environmental impact. As a remedy, we propose a climate performance model card with the primary purpose of being practically usable with only limited information about experiments and the underlying computer hardware. We describe why this step is essential to increase awareness about the environmental impact of NLP research and, thereby, paving the way for more thorough discussions.
                                </div>
                            </div>
                        </li>

                        <li><a id="mcwq"></a>
                            <div class="row">
                                <div class="col">
                                    <b><i>Compositional Generalization in Multilingual Semantic Parsing over Wikidata</i></b>.
                                    <br>Ruixiang Cui, Rahul Aralikatte, Heather Lent and <b>Daniel Hershcovich</b>.
                                    <a href="https://transacl.org/">TACL</a>, 2022.
                                </div>
                                <div class="col btn-group mr-2 v-center" role="group">
                                    <a href="https://aclanthology.org/2022.tacl-1.55/" class="btn btn-outline-primary btn-sm">proceedings</a>
                                    <a href="https://direct.mit.edu/tacl/article/doi/10.1162/tacl_a_00499/112914/Compositional-Generalization-in-Multilingual" class="btn btn-outline-primary btn-sm">MIT Press</a>
                                    <a href="https://arxiv.org/abs/2108.03509" class="btn btn-outline-primary btn-sm">preprint</a>
                                    <a href="#" class="btn btn-outline-primary btn-sm"
                                       onclick="$('#mcwq_abstract').slideToggle(); $(this).toggleClass('active'); return false;">abstract</a>
                                    <a href="2022.tacl-1.55.bib" class="btn btn-outline-primary btn-sm bib">bib</a>
                                    <a href="https://github.com/coastalcph/seq2sparql"
                                       class="btn btn-outline-primary btn-sm">code and data</a>
                                </div>
                                <div id="mcwq_abstract" class="abstract row" style="display:none;">
                                    <i>Semantic parsing (SP) allows humans to leverage vast knowledge resources through natural interaction. However, parsers are mostly designed for and evaluated on English resources, such as CFQ (Keysers et al., 2020), the current standard benchmark based on English data generated from grammar rules and oriented towards Freebase, an outdated knowledge base. We propose a method for creating a multilingual, parallel dataset of question-query pairs, grounded in Wikidata. We introduce such a dataset, which we call Multilingual Compositional Wikidata Questions (MCWQ), and use it to analyze the compositional generalization of semantic parsers in Hebrew, Kannada, Chinese and English. While within-language generalization is comparable across languages, experiments on zero-shot cross-lingual transfer demonstrate that cross-lingual compositional generalization fails, even with state-of-the-art pretrained multilingual encoders. Furthermore, our methodology, dataset and results will facilitate future research on SP in more realistic and diverse settings than has been possible with existing resources.
                                    </i>
                                </div>
                            </div>
                        </li>

                        <li><a id="gqnli"></a>
                            <div class="row">
                                <div class="col">
                                    <b><i>Generalized Quantifiers as a Source of Error in Multilingual NLU Benchmarks</i></b>.
                                    <br>
                                    Ruixiang Cui, <b>Daniel Hershcovich</b> and Anders Søgaard.
                                    <a href="https://2022.naacl.org/">NAACL 2022</a> (long paper).
                                </div>
                                <div class="col btn-group mr-2 v-center" role="group">
                                    <a href="https://aclanthology.org/2022.naacl-main.359/" class="btn btn-outline-primary btn-sm">proceedings</a>
                                    <a href="https://arxiv.org/abs/2204.10615" class="btn btn-outline-primary btn-sm">preprint</a>
                                    <a href="#" class="btn btn-outline-primary btn-sm"
                                       onclick="$('#gqnli_abstract').slideToggle(); $(this).toggleClass('active'); return false;">abstract</a>
                                    <a href="2022.naacl-main.359.bib" class="btn btn-outline-primary btn-sm bib">bib</a>
                                </div>
                                <div id="gqnli_abstract" class="abstract row" style="display:none;">
                                    Logical approaches to representing language have developed and evaluated computational models of quantifier words since the 19th century, but today's NLU models still struggle to capture their semantics. We rely on Generalized Quantifier Theory for language-independent representations of the semantics of quantifier words, to quantify their contribution to the errors of NLU models. We find that quantifiers are pervasive in NLU benchmarks, and their occurrence at test time is associated with performance drops. Multilingual models also exhibit unsatisfying quantifier reasoning abilities, but not necessarily worse for non-English languages. To facilitate directly-targeted probing, we present an adversarial generalized quantifier NLI task (GQNLI) and show that pre-trained language models have a clear lack of robustness in generalized quantifier reasoning.
                                </div>
                            </div>
                        </li>

                        <li><a id="taylor"></a>
                            <div class="row">
                                <div class="col">
                                    <b><i>Evaluating Deep Taylor Decomposition for Reliability Assessment in the Wild</i></b>.
                                    <br>
                                    Stephanie Brandl, <b>Daniel Hershcovich</b> and Anders Søgaard.
                                    <a href="https://www.icwsm.org/2022/">ICWSM 2022</a>.
                                </div>
                                <div class="col btn-group mr-2 v-center" role="group">
                                    <a href="https://ojs.aaai.org/index.php/ICWSM/article/view/19389/19161" class="btn btn-outline-primary btn-sm">proceedings</a>
                                    <a href="https://arxiv.org/abs/2206.02661" class="btn btn-outline-primary btn-sm">preprint</a>
                                    <a href="#" class="btn btn-outline-primary btn-sm"
                                       onclick="$('#taylor_abstract').slideToggle(); $(this).toggleClass('active'); return false;">abstract</a>
                                    <a href="taylor.bib" class="btn btn-outline-primary btn-sm bib">bib</a>
                                </div>
                                <div id="taylor_abstract" class="abstract row" style="display:none;">
                                    We argue that we need to evaluate model interpretability methods in the wild, in situations where professionals make critical decisions, and models can potentially assist them. We present an in-the-wild evaluation of token attribution based on Deep Taylor Decomposition, with professional journalists performing reliability assessments. We find that using this method in conjunction with RoBERTa-Large, fine-tuned on the Gossip Corpus, led to faster and better human decision-making, as well as a more critical attitude toward news sources among the journalists. We present a comparison of human and model rationales, as well as a qualitative analysis of the journalists' experiences with machine-in-the-loop decision making.
                                </div>
                            </div>
                        </li>

                        <li><a id="xculture"></a>
                            <div class="row">
                                <div class="col">
                                    <b><i>Challenges and Strategies in Cross-Cultural NLP</i></b>.
                                    <br>
                                    <b>Daniel Hershcovich</b>, Stella Frank, Heather Lent, Miryam de Lhoneux, Mostafa Abdou, Stephanie Brandl, Emanuele Bugliarello, Laura Cabello Piqueras, Ilias Chalkidis, Ruixiang Cui, Constanza Fierro, Katerina Margatina, Phillip Rust and Anders Søgaard.
                                    <a href="https://2022.aclweb.org/">ACL 2022</a> (long paper).
                                </div>
                                <div class="col btn-group mr-2 v-center" role="group">
                                    <a href="https://aclanthology.org/2022.acl-long.482/" class="btn btn-outline-primary btn-sm">proceedings</a>
                                    <a href="xculture.pdf" class="btn btn-outline-primary btn-sm">pdf</a>
                                    <a href="https://arxiv.org/abs/2203.10020" class="btn btn-outline-primary btn-sm">preprint</a>
                                    <a href="#" class="btn btn-outline-primary btn-sm"
                                       onclick="$('#xculture_abstract').slideToggle(); $(this).toggleClass('active'); return false;">abstract</a>
                                    <a href="2022.acl-long.482.bib" class="btn btn-outline-primary btn-sm bib">bib</a>
                                    <a href="acl2022_xculture_slides.pdf" class="btn btn-outline-primary btn-sm">slides</a>
                                    <a href="acl2022_xculture_poster.pdf" class="btn btn-outline-primary btn-sm">poster</a>
                                </div>
                                <div id="xculture_abstract" class="abstract row" style="display:none;">
                                    Various efforts in the Natural Language Processing (NLP) community have been made to accommodate linguistic diversity and serve speakers of many different languages. However, it is important to acknowledge that speakers and the content they produce and require, vary not just by language, but also by culture. Although language and culture are tightly linked, there are important differences. Analogous to cross-lingual and multilingual NLP, cross-cultural and multicultural NLP considers these differences in order to better serve users of NLP systems. We propose a principled framework to frame these efforts, and survey existing and potential strategies.
                                </div>
                            </div>
                        </li>

                        <li><a id="inspiration"></a>
                            <div class="row">
                                <div class="col">
                                    <b><i>Scaling Creative Inspiration with Fine-Grained Functional Facets of Product Ideas</i></b>.
                                    <br>
                                    Tom Hope, Ronen Tamari, Hyeonsu Kang, <b>Daniel Hershcovich</b>, Joel Chan, Aniket Kittur and Dafna Shahaf.
                                    <a href="https://chi2022.acm.org/">CHI 2022</a>.
                                </div>
                                <div class="col btn-group mr-2 v-center" role="group">
                                    <a href="https://dl.acm.org/doi/10.1145/3491102.3517434"
                                       class="btn btn-outline-primary btn-sm">proceedings</a>
                                    <a href="https://arxiv.org/abs/2102.09761" class="btn btn-outline-primary btn-sm">preprint</a>
                                    <a href="#" class="btn btn-outline-primary btn-sm"
                                       onclick="$('#inspiration_abstract').slideToggle(); $(this).toggleClass('active'); return false;">abstract</a>
                                    <a href="inspiration.bib" class="btn btn-outline-primary btn-sm bib">bib</a>
                                </div>
                                <div id="inspiration_abstract" class="abstract row" style="display:none;">
                                    <i>Web-scale repositories of products, patents and scientific papers offer an opportunity for creating automated systems that scour millions of ideas and assist users in discovering inspirations and solutions. Yet the common representation of ideas is in the form of raw textual descriptions, lacking important structure that is required for supporting creative innovation. Prior work has pointed to the importance of functional structure -- capturing the mechanisms and purposes of inventions -- for allowing users to discover structural connections across ideas and creatively adapt existing technologies. However, the use of functional representations was either coarse and limited in expressivity, or dependent on curated knowledge bases with poor coverage and significant manual effort from users.
                                        To help bridge this gap and unlock the potential of large-scale idea mining, we propose a novel computational representation that automatically breaks up products into fine-grained functional facets. We train a model to extract these facets from a challenging real-world corpus of invention descriptions, and represent each product as a set of facet embeddings. We design similarity metrics that support granular matching between functional facets across ideas, and use them to build a novel functional search capability that enables expressive queries for mechanisms and purposes. We construct a graph capturing hierarchical relations between purposes and mechanisms across an entire corpus of products, and use the graph to help problem-solvers explore the design space around a focal problem and view related problem perspectives. In empirical user studies, our approach leads to a significant boost in search accuracy and in the quality of creative inspirations, outperforming strong baselines and state-of-art representations of product texts by 50-60%.</i>
                                </div>
                            </div>
                        </li>

                        <li><a id="conll21-negation"></a>
                            <div class="row">
                                <div class="col">
                                    <b><i>A Multilingual Benchmark for Probing Negation-Awareness with Minimal Pairs</i></b>.
                                    <br>Mareike Hartmann, Miryam de Lhoneux, <b>Daniel Hershcovich</b>, Yova Kementchedjhieva, Lukas Nielsen, Chen Qiu and Anders Søgaard.<br>
                                    <a href='https://conll.org/2021' target="_blank">CoNLL 2021</a>.
                                </div>
                                <div class="col btn-group mr-2 v-center" role="group">
                                    <a href="https://aclanthology.org/2021.conll-1.19/"
                                       class="btn btn-outline-primary btn-sm">proceedings</a>
                                    <a href="#" class="btn btn-outline-primary btn-sm"
                                       onclick="$('#conll21-negation_abstract').slideToggle(); $(this).toggleClass('active'); return false;">abstract</a>
                                    <a href="conll21-negation.bib" class="btn btn-outline-primary btn-sm bib">bib</a>
                                </div>
                                <div id="conll21-negation_abstract" class="abstract row" style="display:none;">
                                    <i>Negation is one of the most fundamental concepts in human cognition and language, and several natural language inference (NLI) probes have been designed to investigate pretrained language models' ability to detect and reason with negation. However, the existing probing datasets are limited to English only, and do not enable controlled probing of performance in the absence or presence of negation. In response, we present a multilingual (English, Bulgarian, German, French and Chinese) benchmark collection of NLI examples that are grammatical and correctly labeled, as a result of manual inspection and editing. We use the benchmark to probe the negation-awareness of multilingual language models and find that models that correctly predict examples with negation cues often fail to correctly predict their counter-examples {\em without} negation cues, even when the cues are irrelevant for semantic inference.</i>
                                </div>
                            </div>
                        </li>

                        <li><a id="conll21-color"></a>
                            <div class="row">
                                <div class="col">
                                    <b><i>Can Language Models Encode Perceptual Structure Without Grounding? A Case Study in Color</i></b>.
                                    <br>Mostafa Abdou, Artur Kulmizev, <b>Daniel Hershcovich</b>, Stella Frank, Ellie Pavlick and Anders Søgaard.<br>
                                    <a href='https://conll.org/2021' target="_blank">CoNLL 2021</a>.
                                </div>
                                <div class="col btn-group mr-2 v-center" role="group">
                                    <a href="https://aclanthology.org/2021.conll-1.9/"
                                       class="btn btn-outline-primary btn-sm">proceedings</a>
                                    <a href="https://arxiv.org/abs/2109.06129" class="btn btn-outline-primary btn-sm">preprint</a>
                                    <a href="#" class="btn btn-outline-primary btn-sm"
                                       onclick="$('#conll21-color_abstract').slideToggle(); $(this).toggleClass('active'); return false;">abstract</a>
                                    <a href="conll21-color.bib" class="btn btn-outline-primary btn-sm bib">bib</a>
                                </div>
                                <div id="conll21-color_abstract" class="abstract row" style="display:none;">
                                    <i>Pretrained language models have been shown to encode relational information, such as the relations between entities or concepts in knowledge-bases -- (Paris, Capital, France). However, simple relations of this type can often be recovered heuristically and the extent to which models implicitly reflect topological structure that is grounded in world, such as perceptual structure, is unknown. To explore this question, we conduct a thorough case study on color. Namely, we employ a dataset of monolexemic color terms and color chips represented in CIELAB, a color space with a perceptually meaningful distance metric.
                                        Using two methods of evaluating the structural alignment of colors in this space with text-derived color term representations, we find significant correspondence. Analyzing the differences in alignment across the color spectrum, we find that warmer colors are, on average, better aligned to the perceptual color space than cooler ones, suggesting an intriguing connection to findings from recent work on efficient communication in color naming. Further analysis suggests that differences in alignment are, in part, mediated by collocationality and differences in syntactic usage, posing questions as to the relationship between color perception and usage and context.</i>
                                </div>
                            </div>
                        </li>

                        <li><a id="iwpt2021"></a>
                            <div class="row">
                                <div class="col">
                                    <b><i>Great Service! Fine-grained Parsing of Implicit Arguments</i></b>.
                                    <br>Ruixiang Cui and <b>Daniel Hershcovich</b>.<br>
                                    <a href='https://iwpt21.sigparse.org/' target="_blank">IWPT 2021</a>.
                                </div>
                                <div class="col btn-group mr-2 v-center" role="group">
                                    <a href="https://aclanthology.org/2021.iwpt-1.7/"
                                       class="btn btn-outline-primary btn-sm">proceedings</a>
                                    <a href="https://arxiv.org/abs/2106.02561" class="btn btn-outline-primary btn-sm">preprint</a>
                                    <a href="#" class="btn btn-outline-primary btn-sm"
                                       onclick="$('#iwpt2021_abstract').slideToggle(); $(this).toggleClass('active'); return false;">abstract</a>
                                    <a href="iwpt2021.bib" class="btn btn-outline-primary btn-sm bib">bib</a>
                                    <a href="https://github.com/ruixiangcui/implicit_parser"
                                       class="btn btn-outline-primary btn-sm">code</a>
                                </div>
                                <div id="iwpt2021_abstract" class="abstract row" style="display:none;">
                                    <i>Broad-coverage meaning representations in NLP mostly focus on explicitly expressed content. More importantly, the scarcity of datasets annotating diverse implicit roles limits empirical studies into their linguistic nuances. For example, in the web review "Great service!", the provider and consumer are implicit arguments of different types. We examine an annotated corpus of fine-grained implicit arguments (Cui and Hershcovich, 2020) by carefully re-annotating it, resolving several inconsistencies. Subsequently, we present the first transition-based neural parser that can handle implicit arguments dynamically, and experiment with two different transition systems on the improved dataset. We find that certain types of implicit arguments are more difficult to parse than others and that the simpler system is more accurate in recovering implicit arguments, despite having a lower overall parsing score, attesting current reasoning limitations of NLP models. This work will facilitate a better understanding of implicit and underspecified language, by incorporating it holistically into meaning representations.</i>
                                </div>
                            </div>
                        </li>

                        <li><a id="streusle-tagger"></a>
                            <div class="row">
                                <div class="col">
                                    <b><i>Lexical Semantic Recognition</i></b>.
                                    <br>
                                    Nelson F. Liu, <b>Daniel Hershcovich</b>, Michael Kranzlein, Nathan Schneider.<br>
                                    <a href='https://multiword.org/mwe2021/' target="_blank">MWE 2021</a>.
                                </div>
                                <div class="col btn-group mr-2 v-center" role="group">
                                    <a href="https://aclanthology.org/2021.mwe-1.6/"
                                       class="btn btn-outline-primary btn-sm">proceedings</a>
                                    <a href="https://arxiv.org/abs/2004.15008" class="btn btn-outline-primary btn-sm">preprint</a>
                                    <a href="#" class="btn btn-outline-primary btn-sm"
                                       onclick="$('#streusle-tagger_abstract').slideToggle(); $(this).toggleClass('active'); return false;">abstract</a>
                                    <a href="streusle-tagger.bib" class="btn btn-outline-primary btn-sm bib">bib</a>
                                    <a href="https://github.com/nelson-liu/lexical-semantic-recognition"
                                       class="btn btn-outline-primary btn-sm">code</a>
                                </div>
                                <div id="streusle-tagger_abstract" class="abstract row" style="display:none;">
                                    <i>In lexical semantics, full-sentence segmentation and segment labeling of various phenomena are generally treated separately, despite their interdependence. We hypothesize that a unified lexical semantic recognition task is an effective way to encapsulate previously disparate styles of annotation, including multiword expression identification / classification and supersense tagging. Using the STREUSLE corpus, we train a neural CRF sequence tagger and evaluate its performance along various axes of annotation. As the label set generalizes that of previous tasks (PARSEME, DiMSUM), we additionally evaluate how well the model generalizes to those test sets, finding that it approaches or surpasses existing models despite training only on STREUSLE. Our work also establishes baseline models and evaluation metrics for integrated and accurate modeling of lexical semantics, facilitating future work in this area.</i>
                                </div>
                            </div>
                        </li>

                        <li><a id="kisurvey"></a>
                            <div class="row">
                                <div class="col">
                                    <b><i>It’s the Meaning That Counts: The State of the Art in NLP and Semantics</i></b>.
                                    <br>
                                    <b>Daniel Hershcovich</b> and Lucia Donatelli.<br>
                                    <a href='https://link.springer.com/journal/13218' target="_blank">KI - Künstliche Intelligenz</a>, 2021.
                                </div>
                                <div class="col btn-group mr-2 v-center" role="group">
                                    <a href="https://rdcu.be/cl0z3" class="btn btn-outline-primary btn-sm">pdf</a>
                                    <a href="http://link.springer.com/article/10.1007/s13218-021-00726-6" class="btn btn-outline-primary btn-sm">online</a>
                                    <a href="#" class="btn btn-outline-primary btn-sm"
                                       onclick="$('#kisurvey_abstract').slideToggle(); $(this).toggleClass('active'); return false;">abstract</a>
                                    <a href="kisurvey.bib" class="btn btn-outline-primary btn-sm bib">bib</a>
                                </div>
                                <div id="kisurvey_abstract" class="abstract row" style="display:none;">
                                    <i>Semantics, the study of meaning, is central to research in Natural Language Processing (NLP) and many other fields connected to Artificial Intelligence. Nevertheless, how semantics is understood in NLP ranges from traditional, formal linguistic definitions based on logic and the principle of compositionality to more applied notions based on grounding meaning in real-world objects and real-time interaction. “Semantic” methods may additionally strive for meaningful representation of language that integrates broader aspects of human cognition and embodied experience, calling into question how adequate a representation of meaning based on linguistic signal alone is for current research agendas. We review the state of computational semantics in NLP and investigate how different lines of inquiry reflect distinct understandings of semantics and prioritize different layers of linguistic meaning. In conclusion, we identify several important goals of the field and describe how current research addresses them.</i>
                                </div>
                            </div>
                        </li>

                        <li><a id="indic2021"></a>
                            <div class="row">
                                <div class="col">
                                    <b><i>How far can we get with one GPU in 100 hours? CoAStaL at MultiIndicMT Shared Task</i></b>.
                                    <br>
                                    Rahul Aralikatte, Héctor Ricardo Murrieta Bello, <b>Daniel Hershcovich</b>, Marcel Bollmann, Anders Søgaard.<br>
                                    <a href='http://lotus.kuee.kyoto-u.ac.jp/WAT/indic-multilingual/' target="_blank">MultiIndicMT Shared Task 2021</a>.
                                </div>
                                <div class="col btn-group mr-2 v-center" role="group">
                                    <a href="https://aclanthology.org/2021.wat-1.24/"
                                       class="btn btn-outline-primary btn-sm">proceedings</a>
                                    <a href="#" class="btn btn-outline-primary btn-sm"
                                       onclick="$('#indic2021_abstract').slideToggle(); $(this).toggleClass('active'); return false;">abstract</a>
                                    <a href="indic2021.bib" class="btn btn-outline-primary btn-sm bib">bib</a>
                                </div>
                                <div id="indic2021_abstract" class="abstract row" style="display:none;">
                                    <i>This work shows that competitive translation results can be obtained in a constrained setting by incorporating the latest advances in memory and compute optimization. We train and evaluate large multilingual translation models using a single GPU for a maximum of 100 hours and get within 4-5 BLEU points of the top submission on the leaderboard. We also benchmark standard baselines on the PMI corpus and re-discover well-known shortcomings of translation systems and metrics.</i>
                                </div>
                            </div>
                        </li>

                        <li><a id="americasnlp2021"></a>
                            <div class="row">
                                <div class="col">
                                    <b><i>Moses and the Character-Based Random Babbling Baseline: CoAStaL at AmericasNLP 2021 Shared Task</i></b>.
                                    <br>
                                    Marcel Bollmann, Rahul Aralikatte, Héctor Murrieta Bello, <b>Daniel Hershcovich</b>, Miryam de Lhoneux, Anders Søgaard.<br>
                                    <a href='http://turing.iimas.unam.mx/americasnlp/index.html' target="_blank">AmericasNLP Shared Task 2021</a>.
                                </div>
                                <div class="col btn-group mr-2 v-center" role="group">
                                    <a href="https://aclanthology.org/2021.americasnlp-1.28/"
                                       class="btn btn-outline-primary btn-sm">proceedings</a>
                                    <a href="#" class="btn btn-outline-primary btn-sm"
                                       onclick="$('#americasnlp2021_abstract').slideToggle(); $(this).toggleClass('active'); return false;">abstract</a>
                                    <a href="americasnlp2021.bib" class="btn btn-outline-primary btn-sm bib">bib</a>
                                </div>
                                <div id="americasnlp2021_abstract" class="abstract row" style="display:none;">
                                    <i>We evaluated a range of neural machine translation techniques developed specifically for low-resource scenarios. Unsuccessfully. In the end, we submitted two runs: (i) a standard phrase-based model, and (ii) a random babbling baseline using character trigrams. We found that it was surprisingly hard to beat (i), in spite of this model being, in theory, a bad fit for polysynthetic languages; and more interestingly, that (ii) was better than several of the submitted systems, highlighting how difficult low-resource machine translation for polysynthetic languages is.</i>
                                </div>
                            </div>
                        </li>

                        <li><a id="debater"></a>
                            <div class="row">
                                <div class="col">
                                    <b><i>An autonomous debating system</i></b>.
                                    <br>
                                    Noam Slonim, Yonatan Bilu, Carlos Alzate, Roy Bar-Haim, Ben Bogin, Francesca Bonin, Leshem Choshen, Edo Cohen-Karlik, Lena Dankin, Lilach Edelstein, Liat Ein-Dor, Roni Friedman-Melamed, Assaf Gavron, Ariel Gera, Martin Gleize, Shai Gretz, Dan Gutfreund, Alon Halfon, <b>Daniel Hershcovich</b>, Ron Hoory, Yufang Hou, Shay Hummel, Michal Jacovi, Charles Jochim, Yoav Kantor, Yoav Katz, David Konopnicki, Zvi Kons, Lili Kotlerman, Dalia Krieger, Dan Lahav, Tamar Lavee, Ran Levy, Naftali Liberman, Yosi Mass, Amir Menczel, Shachar Mirkin, Guy Moshkowich, Shila Ofek-Koifman, Matan Orbach, Ella Rabinovich, Ruty Rinott, Slava Shechtman, Dafna Sheinwald, Eyal Shnarch, Ilya Shnayderman, Aya Soffer, Artem Spector, Benjamin Sznajder, Assaf Toledo, Orith Toledo-Ronen, Elad Venezian and Ranit Aharonov.<br>
                                    <a href="https://www.nature.com/nature">Nature</a>, 2021.
                                </div>
                                <div class="col btn-group mr-2 v-center" role="group">
                                    <a href="https://eorder.sheridan.com/3_0/app/orders/11030/files/assets/common/downloads/Slonim.pdf" class="btn btn-outline-primary btn-sm">pdf</a>
                                    <a href="https://www.nature.com/articles/s41586-021-03215-w" class="btn btn-outline-primary btn-sm">online</a>
                                    <a href="#" class="btn btn-outline-primary btn-sm"
                                       onclick="$('#debater_abstract').slideToggle(); $(this).toggleClass('active'); return false;">abstract</a>
                                    <a href="debater.bib" class="btn btn-outline-primary btn-sm bib">bib</a>
                                    <a href="https://www.research.ibm.com/artificial-intelligence/project-debater/" class="btn btn-outline-primary btn-sm">website</a>
                                </div>
                                <div id="debater_abstract" class="abstract row" style="display:none;">
                                    <i>Artificial intelligence (AI) is defined as the ability of machines to perform tasks that are usually associated with intelligent beings. Argument and debate are fundamental capabilities of human intelligence, essential for a wide range of human activities, and common to all human societies. The development of computational argumentation technologies is therefore an important emerging discipline in AI research1. Here we present Project Debater, an autonomous debating system that can engage in a competitive debate with humans. We provide a complete description of the system’s architecture, a thorough and systematic evaluation of its operation across a wide range of debate topics, and a detailed account of the system’s performance in its public debut against three expert human debaters. We also highlight the fundamental differences between debating with humans as opposed to challenging humans in game competitions, the latter being the focus of classical ‘grand challenges’ pursued by the AI research community over the past few decades. We suggest that such challenges lie in the ‘comfort zone’ of AI, whereas debating with humans lies in a different territory, in which humans still prevail, and for which novel paradigms are required to make substantial progress.</i>
                                </div>
                            </div>
                        </li>

                        <li><a id="srl-coref-rl"></a>
                            <div class="row">
                                <div class="col">
                                    <b><i>Joint Semantic Analysis with Document-Level Cross-Task Coherence Rewards</i></b>.
                                    <br>
                                    Rahul Aralikatte, Mostafa Abdou, Heather Lent, <b>Daniel Hershcovich</b> and Anders Søgaard.<br>
                                    <a href='https://aaai.org/Conferences/AAAI-21' target="_blank">AAAI 2021</a> (long paper).
                                </div>
                                <div class="col btn-group mr-2 v-center" role="group">
                                    <a href="https://arxiv.org/abs/2010.05567" class="btn btn-outline-primary btn-sm">preprint</a>
                                    <a href="#" class="btn btn-outline-primary btn-sm"
                                       onclick="$('#srl_coref_rl_abstract').slideToggle(); $(this).toggleClass('active'); return false;">abstract</a>
                                    <a href="srl-coref-rl.bib" class="btn btn-outline-primary btn-sm bib">bib</a>
                                    <a href="https://github.com/rahular/joint-coref-srl"
                                       class="btn btn-outline-primary btn-sm">code</a>
                                </div>
                                <div id="srl_coref_rl_abstract" class="abstract row" style="display:none;">
                                    <i>Coreference resolution and semantic role labeling are NLP tasks that capture different aspects of semantics, indicating respectively, which expressions refer to the same entity, and what semantic roles expressions serve in the sentence. However, they are often closely interdependent, and both generally necessitate natural language understanding. Do they form a coherent abstract representation of documents? We present a neural network architecture for joint coreference resolution and semantic role labeling for English, and train graph neural networks to model the 'coherence' of the combined shallow semantic graph. Using the resulting coherence score as a reward for our joint semantic analyzer, we use reinforcement learning to encourage global coherence over the document and between semantic annotations. This leads to improvements on both tasks in multiple datasets from different domains, and across a range of encoders of different expressivity, calling, we believe, for a more holistic approach for semantics in NLP.
                                    </i>
                                </div>
                            </div>
                        </li>

                        <li><a id="dmr2020"></a>
                            <div class="row">
                                <div class="col">
                                    <b><i>Refining Implicit Argument Annotation For UCCA</i></b>.
                                    <br>Ruixiang Cui and <b>Daniel Hershcovich</b>.<br>
                                    <a href='https://www.cs.brandeis.edu/~clp/dmr2020/' target="_blank">DMR 2020</a>.
                                </div>
                                <div class="col btn-group mr-2 v-center" role="group">
                                    <a href="dmr2020.pdf" class="btn btn-outline-primary btn-sm">pdf</a>
                                    <a href="https://www.aclweb.org/anthology/2020.dmr-1.5/"
                                       class="btn btn-outline-primary btn-sm">proceedings</a>
                                    <a href="https://arxiv.org/abs/2005.12889" class="btn btn-outline-primary btn-sm">preprint</a>
                                    <a href="#" class="btn btn-outline-primary btn-sm"
                                       onclick="$('#dmr2020_abstract').slideToggle(); $(this).toggleClass('active'); return false;">abstract</a>
                                    <a href="dmr2020.bib" class="btn btn-outline-primary btn-sm bib">bib</a>
                                    <a href="dmr2020_slides.pdf" class="btn btn-outline-primary btn-sm">slides</a>
                                    <a href="https://github.com/ruixiangcui/UCCA-Refined-Implicit-EWT_English"
                                       class="btn btn-outline-primary btn-sm">data</a>
                                </div>
                                <div id="dmr2020_abstract" class="abstract row" style="display:none;">
                                    <i>Predicate-argument structure analysis is a central
                                        component in meaning representations of text. The
                                        fact that some arguments are not explicitly mentioned
                                        in a sentence gives rise to ambiguity in language
                                        understanding, and renders it difficult for machines to
                                        interpret text correctly. However, only few resources
                                        represent implicit roles for NLU, and existing studies in
                                        NLP only make coarse distinctions between categories of
                                        arguments omitted from linguistic form. To better
                                        understand the behaviour of implicit roles and their
                                        characteristics, in this paper, we design a typology for
                                        fine-grained implicit argument annotation on top of Universal
                                        Conceptual Cognitive Annotation's foundational layer. The
                                        proposed implicit argument categorisation is driven by theories
                                        of implicit role interpretation and consists of six types:
                                        Deictic, Generic, Genre-based, Type-identifiable, Non-specific, and
                                        Iterated-set. We exemplify our design by revisiting part of the
                                        UCCA EWT corpus, providing a new dataset annotated with the
                                        refinement layer, and making a comparative analysis with other
                                        schemes both in terms of quantity and quality.</i>
                                </div>
                            </div>
                        </li>

                        <li><a id="streusle2ucca"></a>
                            <div class="row">
                                <div class="col">
                                    <b><i>Comparison by Conversion: Reverse-Engineering UCCA from Syntax and Lexical
                                        Semantics</i></b>.
                                    <br>
                                    <b>Daniel Hershcovich</b>, Nathan Schneider, Dotan Dvir, Jakob Prange, Miryam de Lhoneux and
                                    Omri Abend.<br>
                                    <a href='https://coling2020.org/' target="_blank">COLING 2020</a>.
                                </div>
                                <div class="col btn-group mr-2 v-center" role="group">
                                    <a href="streusle2ucca.pdf" class="btn btn-outline-primary btn-sm">pdf</a>
                                    <a href="https://www.aclweb.org/anthology/2020.coling-main.264/"
                                       class="btn btn-outline-primary btn-sm">proceedings</a>
                                    <a href="https://arxiv.org/abs/2011.00834" class="btn btn-outline-primary btn-sm">preprint</a>
                                    <a href="#" class="btn btn-outline-primary btn-sm"
                                       onclick="$('#streusle2ucca_abstract').slideToggle(); $(this).toggleClass('active'); return false;">abstract</a>
                                    <a href="streusle2ucca.bib" class="btn btn-outline-primary btn-sm bib">bib</a>
                                    <a href="streusle2ucca_poster.pdf" class="btn btn-outline-primary btn-sm">poster</a>
                                    <a href="https://github.com/danielhers/streusle/tree/streusle2ucca"
                                       class="btn btn-outline-primary btn-sm">converter code</a>
                                    <a href="https://github.com/danielhers/hit-scir-ucca-parser"
                                       class="btn btn-outline-primary btn-sm">parser code</a>
                                </div>
                                <div id="streusle2ucca_abstract" class="abstract row" style="display:none;">
                                    <i>Building robust natural language understanding systems will require a clear characterization
                                        of whether and how various linguistic meaning representations complement each other. To
                                        perform a systematic comparative analysis, we evaluate the mapping between meaning
                                        representations from different frameworks using two complementary methods: (i) a rule-based
                                        converter, and (ii) a supervised delexicalized parser that parses to one framework
                                        using only information from the other as features. We apply these methods to convert the
                                        STREUSLE corpus (with syntactic and lexical semantic annotations) to UCCA (a
                                        graph-structured full-sentence meaning representation). Both methods yield surprisingly
                                        accurate target representations, close to fully supervised UCCA parser quality---indicating
                                        that UCCA annotations are partially redundant with STREUSLE annotations. Despite this
                                        substantial convergence between frameworks, we find several important areas of divergence.
                                    </i>
                                </div>
                            </div>
                        </li>

                        <li><a id="mrp2020"></a>
                            <div class="row">
                                <div class="col">
                                    <b><i>MRP 2020: The Second Shared Task on Cross-Framework and
                                        Cross-Lingual Meaning Representation Parsing</i></b>.
                                    <br>
                                    Stephan Oepen, Omri Abend, Lasha Abzianidze, Johan Bos,
                                    Jan Hajic, <b>Daniel Hershcovich</b>, Bin Li, Tim
                                    O'Gorman, Nianwen Xue and Daniel Zeman.<br>
                                    <a href='https://www.conll.org/2020' target="_blank">CoNLL 2020</a> shared task.
                                </div>
                                <div class="col btn-group mr-2 v-center" role="group">
                                    <a href="mrp2020.pdf" class="btn btn-outline-primary btn-sm">pdf</a>
                                    <a href="https://www.aclweb.org/anthology/2020.conll-shared.1/"
                                       class="btn btn-outline-primary btn-sm">proceedings</a>
                                    <a href="#" class="btn btn-outline-primary btn-sm"
                                       onclick="$('#mrp2020_abstract').slideToggle(); $(this).toggleClass('active'); return false;">abstract</a>
                                    <a href="mrp2020.bib" class="btn btn-outline-primary btn-sm bib">bib</a>
                                    <a href="mrp2020_slides.pdf" class="btn btn-outline-primary btn-sm">slides</a>
                                    <a href="http://mrp.nlpl.eu/2020" class="btn btn-outline-primary btn-sm">website</a>
                                </div>
                                <div id="mrp2020_abstract" class="abstract row" style="display:none;">
                                    <i>The 2020 Shared Task at the Conference for Computational Language
                                        Learning (CoNLL) was devoted to Meaning Representation Parsing (MRP)
                                        across frameworks and languages.
                                        Extending a similar setup from the previous year,
                                        five distinct approaches to the representation of sentence meaning in
                                        the form of directed graphs were represented in the English training and
                                        evaluation data for the task, packaged in a uniform graph abstraction
                                        and serialization;
                                        for four of these representation frameworks, additional training and
                                        evaluation data was provided for one additional language per framework.
                                        The task received submissions from eight teams, of which two do
                                        not participate in the official ranking because they arrived after
                                        the closing deadline or made use of additional training data.
                                        All technical information regarding the task, including system
                                        submissions, official results, and links to supporting resources and
                                        software are available from the task web site at: http://mrp.nlpl.eu
                                    </i>
                                </div>
                            </div>
                        </li>

                        <li><a id="huji-ku-mrp2020"></a>
                            <div class="row">
                                <div class="col">
                                    <b><i>HUJI-KU at MRP 2020: Two Transition-based Neural Parsers</i></b>.
                                    <br>Ofir Arviv, Ruixiang Cui and <b>Daniel Hershcovich</b>.<br>
                                    <a href='https://www.conll.org/2020' target="_blank">CoNLL 2020</a> shared task.
                                </div>
                                <div class="col btn-group mr-2 v-center" role="group">
                                    <a href="huji-ku-mrp.pdf" class="btn btn-outline-primary btn-sm">pdf</a>
                                    <a href="https://arxiv.org/abs/2010.05710" class="btn btn-outline-primary btn-sm">preprint</a>
                                    <a href="https://www.aclweb.org/anthology/2020.conll-shared.7/"
                                       class="btn btn-outline-primary btn-sm">proceedings</a>
                                    <a href="#" class="btn btn-outline-primary btn-sm"
                                       onclick="$('#huji-ku-mrp2020_abstract').slideToggle(); $(this).toggleClass('active'); return false;">abstract</a>
                                    <a href="huji-ku-mrp2020.bib" class="btn btn-outline-primary btn-sm bib">bib</a>
                                    <a href="huji-ku-mrp_slides.pdf" class="btn btn-outline-primary btn-sm">slides</a>
                                    <a href="https://github.com/danielhers/tupa/tree/mrp"
                                       class="btn btn-outline-primary btn-sm">TUPA code</a>
                                    <a href="https://github.com/ruixiangcui/hit-scir-mrp2020"
                                       class="btn btn-outline-primary btn-sm">HIT-SCIR code</a>
                                    <a href="https://github.com/OfirArviv/hit-scir-mrp2020/tree/multitask"
                                       class="btn btn-outline-primary btn-sm">multitask code</a>
                                </div>
                                <div id="huji-ku-mrp2020_abstract" class="abstract row" style="display:none;">
                                    <i>This paper describes the HUJI-KU system submission to the shared task
                                        on Cross-Framework Meaning Representation Parsing (MRP) at the 2020
                                        Conference for Computational Language Learning (CoNLL),
                                        employing TUPA and the HIT-SCIR parser, which were, respectively,
                                        the baseline system and winning system in the 2019 MRP shared task.
                                        Both are transition-based parsers using BERT contextualized embeddings.
                                        We generalized TUPA to support the newly-added MRP frameworks and languages,
                                        and experimented with multitask learning with the HIT-SCIR parser.
                                        We reached 4th place in both the cross-framework and cross-lingual tracks.</i>
                                </div>
                            </div>
                        </li>

                        <li><a id="iwptst2020"></a>
                            <div class="row">
                                <div class="col">
                                    <b><i>Køpsala: Transition-Based Graph Parsing via Efficient Training and Effective Encoding</i></b>.
                                    <br>
                                    <b>Daniel Hershcovich</b>, Miryam de Lhoneux, Artur Kulmizev, Elham Pejhan and Joakim Nivre.<br>
                                    <a href='https://universaldependencies.org/iwpt20' target="_blank">IWPT 2020 shared task</a>.
                                </div>
                                <div class="col btn-group mr-2 v-center" role="group">
                                    <a href="iwptst2020.pdf" class="btn btn-outline-primary btn-sm">pdf</a>
                                    <a href="https://arxiv.org/abs/2005.12094" class="btn btn-outline-primary btn-sm">preprint</a>
                                    <a href="https://www.aclweb.org/anthology/2020.iwpt-1.25/"
                                       class="btn btn-outline-primary btn-sm">proceedings</a>
                                    <a href="#" class="btn btn-outline-primary btn-sm"
                                       onclick="$('#iwptst2020_abstract').slideToggle(); $(this).toggleClass('active'); return false;">abstract</a>
                                    <a href="iwptst2020.bib" class="btn btn-outline-primary btn-sm bib">bib</a>
                                    <a href="iwptst2020_slides.pdf" class="btn btn-outline-primary btn-sm">slides</a>
                                    <a href="https://github.com/coastalcph/koepsala-parser" class="btn btn-outline-primary btn-sm">code</a>
                                </div>
                                <div id="iwptst2020_abstract" class="abstract row" style="display:none;">
                                    <i>We present Køpsala, the Copenhagen-Uppsala system for the Enhanced Universal Dependencies
                                        Shared Task at IWPT 2020. Our system is a pipeline consisting of off-the-shelf models for
                                        everything but enhanced graph parsing, and for the latter, a transition-based graph parser
                                        adapted from Che et al. (2019). We train a single enhanced parser model per language, using
                                        gold sentence splitting and tokenization for training, and rely only on tokenized surface
                                        forms and multilingual BERT for encoding. While a bug introduced just before submission
                                        resulted in a severe drop in precision, its post-submission fix would bring us to 4th place
                                        in the official ranking, according to average ELAS. Our parser demonstrates that a unified
                                        pipeline is effective for both Meaning Representation Parsing and Enhanced Universal
                                        Dependencies.</i>
                                </div>
                            </div>
                        </li>

                        <li><a id="mrp2019"></a>
                            <div class="row">
                                <div class="col">
                                    <b><i>MRP 2019: Cross-Framework Meaning Representation Parsing</i></b>.
                                    <br>
                                    Stephan Oepen, Omri Abend, Jan Hajič, <b>Daniel Hershcovich</b>,
                                    Marco Kuhlmann, Tim O'Gorman, Nianwen Xue, Jayeol Chun, Milan Straka and Zdeňka Urešová.<br>
                                    <a href='https://www.conll.org/2019' target="_blank">CoNLL 2019</a> shared task.
                                </div>
                                <div class="col btn-group mr-2 v-center" role="group">
                                    <a href="http://svn.nlpl.eu/mrp/2019/public/overview.pdf"
                                       class="btn btn-outline-primary btn-sm">pdf</a>
                                    <a href="https://www.aclweb.org/anthology/K19-2001" class="btn btn-outline-primary btn-sm">proceedings</a>
                                    <a href="#" class="btn btn-outline-primary btn-sm"
                                       onclick="$('#mrp2019_abstract').slideToggle(); $(this).toggleClass('active'); return false;">abstract</a>
                                    <a href="mrp2019.bib" class="btn btn-outline-primary btn-sm bib">bib</a>
                                    <a href="http://svn.nlpl.eu/mrp/2019/public/slides.pdf" class="btn btn-outline-primary btn-sm">slides</a>
                                    <a href="http://mrp.nlpl.eu/2019" class="btn btn-outline-primary btn-sm">website</a>
                                </div>
                                <div id="mrp2019_abstract" class="abstract row" style="display:none;">
                                    <i>The 2019 Shared Task at the Conference for
                                        Computational Language Learning (CoNLL) was devoted
                                        to Meaning Representation Parsing (MRP) across
                                        frameworks. Five distinct approaches to the
                                        representation of sentence meaning in the form of
                                        directed graph were represented in the training and
                                        evaluation data for the task, packaged in a uniform
                                        abstract graph representation and serialization. The
                                        task received submissions from eighteen teams, of
                                        which five do not participate in the official ranking
                                        because they arrived after the closing deadline, made
                                        use of additional training data, or involved one of
                                        the task co-organizers. All technical information
                                        regarding the task, including system submissions,
                                        official results, and links to supporting resources
                                        and software are available from the task web site at:
                                        http://mrp.nlpl.eu
                                    </i>
                                </div>
                            </div>
                        </li>

                        <li><a id="tupa-mrp"></a>
                            <div class="row">
                                <div class="col">
                                    <b><i>TUPA at MRP 2019: A Multi-Task Baseline System</i></b>.
                                    <br>
                                    <b>Daniel Hershcovich</b> and Ofir Arviv.<br>
                                    <a href='https://www.conll.org/2019' target="_blank">CoNLL 2019</a> shared task.
                                </div>
                                <div class="col btn-group mr-2 v-center" role="group">
                                    <a href="D19-6802.pdf" class="btn btn-outline-primary btn-sm">pdf</a>
                                    <a href="https://www.aclweb.org/anthology/K19-2002" class="btn btn-outline-primary btn-sm">proceedings</a>
                                    <a href="#" class="btn btn-outline-primary btn-sm"
                                       onclick="$('#tupa_mrp_abstract').slideToggle(); $(this).toggleClass('active'); return false;">abstract</a>
                                    <a href="tupa-mrp.bib" class="btn btn-outline-primary btn-sm bib">bib</a>
                                    <a href="tupa-mrp-poster.pdf" class="btn btn-outline-primary btn-sm">poster</a>
                                    <a href="tupa-mrp-slides.pdf" class="btn btn-outline-primary btn-sm">slides</a>
                                    <a href="https://github.com/danielhers/tupa/tree/mrp"
                                       class="btn btn-outline-primary btn-sm">code</a>
                                </div>
                                <div id="tupa_mrp_abstract" class="abstract row" style="display:none;">
                                    <i>This paper describes the TUPA system
                                        submission to the shared task on CrossFramework Meaning Representation Parsing
                                        (MRP) at the 2019 Conference for Computational Language Learning (CoNLL). TUPA
                                        provides a baseline point of comparison and is
                                        not considered in the official ranking of participating systems. While originally developed
                                        for UCCA only, TUPA has been generalized
                                        to support all MRP frameworks included in
                                        the task, and trained using multi-task learning
                                        to parse them all with a shared model. It is
                                        a transition-based parser with a BiLSTM encoder, augmented with BERT contextualized
                                        embeddings.
                                    </i>
                                </div>
                            </div>
                        </li>

                        <li><a id="coref-rl"></a>
                            <div class="row">
                                <div class="col">
                                    <b><i>Rewarding Coreference Resolvers for Being Consistent with World Knowledge</i></b>.
                                    <br>
                                    Rahul Aralikatte, Heather Lent, Ana Valeria Gonzalez, <b>Daniel Hershcovich</b>, Chen Qiu,
                                    Anders Sandholm, Michael Ringgaard and Anders Søgaard.<br>
                                    <a href='https://emnlp-ijcnlp2019.org' target="_blank">EMNLP-IJCNLP 2019</a> (short paper).
                                </div>
                                <div class="col btn-group mr-2 v-center" role="group">
                                    <a href="coref-rl.pdf" class="btn btn-outline-primary btn-sm">pdf</a>
                                    <a href="https://arxiv.org/abs/1909.02392" class="btn btn-outline-primary btn-sm">preprint</a>
                                    <a href="https://www.aclweb.org/anthology/D19-1118" class="btn btn-outline-primary btn-sm">proceedings</a>
                                    <a href="#" class="btn btn-outline-primary btn-sm"
                                       onclick="$('#coref_rl_abstract').slideToggle(); $(this).toggleClass('active'); return false;">abstract</a>
                                    <a href="coref-rl.bib" class="btn btn-outline-primary btn-sm bib">bib</a>
                                    <a href="https://github.com/rahular/coref-rl"
                                       class="btn btn-outline-primary btn-sm">code</a>
                                </div>
                                <div id="coref_rl_abstract" class="abstract row" style="display:none;">
                                    <i>Unresolved coreference is a bottleneck for relation
                                        extraction, and high-quality coreference resolvers
                                        may produce an output that makes it a lot easier to
                                        extract knowledge triples. We show how to improve
                                        coreference resolvers by forwarding their input to a
                                        relation extraction system and reward the resolvers
                                        for producing triples that are found in knowledge
                                        bases. Since relation extraction systems can rely on
                                        different forms of supervision and be biased in
                                        different ways, we obtain the best performance,
                                        improving over the state of the art, using multi-task
                                        reinforcement learning.
                                    </i>
                                </div>
                            </div>
                        </li>

                        <li><a id="cyber"></a>
                            <div class="row">
                                <div class="col">
                                    <b><i>The Language of Legal and Illegal Activity on the Darknet</i></b>.
                                    <br>
                                    Leshem Choshen*, Dan Eldad*, <b>Daniel Hershcovich</b>*, Elior Sulem* and Omri Abend.<br>
                                    <a href='http://acl2019.org' target="_blank">ACL 2019</a> (long paper).
                                </div>
                                <div class="col btn-group mr-2 v-center" role="group">
                                    <a href="P19-1419.pdf" class="btn btn-outline-primary btn-sm">pdf</a>
                                    <a href="https://arxiv.org/abs/1905.05543" class="btn btn-outline-primary btn-sm">preprint</a>
                                    <a href="https://www.aclweb.org/anthology/P19-1419" class="btn btn-outline-primary btn-sm">proceedings</a>
                                    <a href="#" class="btn btn-outline-primary btn-sm"
                                       onclick="$('#cyber_abstract').slideToggle(); $(this).toggleClass('active'); return false;">abstract</a>
                                    <a href="P19-1419.bib" class="btn btn-outline-primary btn-sm bib">bib</a>
                                    <a href="cyber_slides.pdf" class="btn btn-outline-primary btn-sm">slides</a>
                                    <a href="https://github.com/huji-nlp/cyber"
                                       class="btn btn-outline-primary btn-sm">code</a>
                                </div>
                                <div id="cyber_abstract" class="abstract row" style="display:none;">
                                    <i>The non-indexed parts of the Internet (the Darknet) have become a haven for both legal and
                                        illegal anonymous activity. Given the magnitude of these networks, scalably monitoring their
                                        activity necessarily relies on automated tools, and notably on NLP tools. However, little is
                                        known about what characteristics texts communicated through the Darknet have, and how well
                                        off-the-shelf NLP tools do on this domain. This paper tackles this gap and performs an
                                        in-depth investigation of the characteristics of legal and illegal text in the Darknet,
                                        comparing it to a clear net website with similar content as a control condition. Taking
                                        drug-related websites as a test case, we find that texts for selling legal and illegal drugs
                                        have several linguistic characteristics that distinguish them from one another, as well as
                                        from the control condition, among them the distribution of POS tags, and the coverage of
                                        their named entities in Wikipedia.
                                    </i>
                                </div>
                            </div>
                        </li>

                        <li><a id="gco"></a>
                            <div class="row">
                                <div class="col">
                                    <b><i>Argument Invention from First Principles</i></b>.
                                    <br>
                                    Yonatan Bilu, Ariel Gera, <b>Daniel Hershcovich</b>, Benjamin Sznajder, Dan Lahav, Guy
                                    Moshkowich, Anael Malet, Assaf Gavron and Noam Slonim.<br>
                                    <a href='http://acl2019.org' target="_blank">ACL 2019</a> (long paper).
                                </div>
                                <div class="col btn-group mr-2 v-center" role="group">
                                    <a href="P19-1097.pdf" class="btn btn-outline-primary btn-sm">pdf</a>
                                    <a href="https://arxiv.org/abs/1908.08336" class="btn btn-outline-primary btn-sm">preprint</a>
                                    <a href="https://www.aclweb.org/anthology/P19-1097" class="btn btn-outline-primary btn-sm">proceedings</a>
                                    <a href="#" class="btn btn-outline-primary btn-sm"
                                       onclick="$('#gco_abstract').slideToggle(); $(this).toggleClass('active'); return false;">abstract</a>
                                    <a href="P19-1097.bib" class="btn btn-outline-primary btn-sm bib">bib</a>
                                </div>
                                <div id="gco_abstract" class="abstract row" style="display:none;">
                                    <i>Competitive debaters often find themselves facing a challenging task -- how to debate a topic
                                        they know very little about, with only minutes to prepare, and without access to books or
                                        the Internet? What they often do is rely on ''first principles'', commonplace arguments
                                        which are relevant to many topics, and which they have refined in past debates. In this work
                                        we aim to explicitly define a taxonomy of such principled recurring arguments, and, given a
                                        controversial topic, to automatically identify which of these arguments are relevant to the
                                        topic. As far as we know, this is the first time that this approach to argument invention is
                                        formalized and made explicit in the context of NLP. The main goal of this work is to show
                                        that it is possible to define such a taxonomy. While the taxonomy suggested here should be
                                        thought of as a ''first attempt'' it is nonetheless coherent, covers well the relevant
                                        topics and coincides with what professional debaters actually argue in their speeches, and
                                        facilitates automatic argument invention for new topics.
                                    </i>
                                </div>
                            </div>
                        </li>

                        <li><a id="semeval2019"></a>
                            <div class="row">
                                <div class="col">
                                    <b><i>SemEval 2019 Task 1: Cross-lingual Semantic Parsing with UCCA</i></b>.
                                    <br>
                                    <b>Daniel Hershcovich</b>, Zohar Aizenbud, Leshem Choshen, Elior Sulem, Ari Rappoport and Omri
                                    Abend.<br>
                                    <a href='http://alt.qcri.org/semeval2019' target="_blank">SemEval 2019</a> shared task.
                                </div>
                                <div class="col btn-group mr-2 v-center" role="group">
                                    <a href="S19-2001.pdf" class="btn btn-outline-primary btn-sm">pdf</a>
                                    <a href="https://arxiv.org/abs/1903.02953" class="btn btn-outline-primary btn-sm">preprint</a>
                                    <a href="https://www.aclweb.org/anthology/S19-2001" class="btn btn-outline-primary btn-sm">proceedings</a>
                                    <a href="#" class="btn btn-outline-primary btn-sm"
                                       onclick="$('#semeval2019_abstract').slideToggle(); $(this).toggleClass('active'); return false;">abstract</a>
                                    <a href="S19-2001.bib" class="btn btn-outline-primary btn-sm bib">bib</a>
                                    <a href="semeval2019_slides.pdf" class="btn btn-outline-primary btn-sm">slides</a>
                                    <a href="https://competitions.codalab.org/competitions/19160"
                                       class="btn btn-outline-primary btn-sm">website</a>
                                </div>
                                <div id="semeval2019_abstract" class="abstract row" style="display:none;">
                                    <i>We present the SemEval 2019 shared task on Universal Conceptual Cognitive Annotation (UCCA)
                                        parsing in English, German and French, and discuss the participating systems and results.
                                        UCCA is a cross-linguistically applicable framework for semantic representation, which
                                        builds on extensive typological work and supports rapid annotation. UCCA poses a challenge
                                        for existing parsing techniques, as it exhibits reentrancy (resulting in DAG structures),
                                        discontinuous structures and non-terminal nodes corresponding to complex semantic units. The
                                        shared task has yielded improvements over the state-of-the-art baseline in all languages and
                                        settings. Full results can be found in the task's website.
                                    </i>
                                </div>
                            </div>
                        </li>

                        <li><a id="repeval2019"></a>
                            <div class="row">
                                <div class="col">
                                    <b><i>Syntactic Interchangeability in Word Embedding Models</i></b>.
                                    <br>
                                    <b>Daniel Hershcovich</b>, Assaf Toledo, Alon Halfon and Noam Slonim.<br>
                                    <a href='https://repeval2019.github.io' target="_blank">RepEval 2019</a>.
                                </div>
                                <div class="col btn-group mr-2 v-center" role="group">
                                    <a href="W19-2009.pdf" class="btn btn-outline-primary btn-sm">pdf</a>
                                    <a href="https://arxiv.org/abs/1904.00669" class="btn btn-outline-primary btn-sm">preprint</a>
                                    <a href="https://www.aclweb.org/anthology/W19-2009" class="btn btn-outline-primary btn-sm">proceedings</a>
                                    <a href="#" class="btn btn-outline-primary btn-sm"
                                       onclick="$('#repeval2019_abstract').slideToggle(); $(this).toggleClass('active'); return false;">abstract</a>
                                    <a href="interchangeability.bib" class="btn btn-outline-primary btn-sm bib">bib</a>
                                    <a href="interchangeability_poster.pdf" class="btn btn-outline-primary btn-sm">poster</a>
                                    <a href="interchangeability_poster_slide.pdf" class="btn btn-outline-primary btn-sm">slide</a>
                                    <a href="https://github.com/danielhers/interchangeability"
                                       class="btn btn-outline-primary btn-sm">code</a>
                                </div>
                                <div id="repeval2019_abstract" class="abstract row" style="display:none;">
                                    <i>Nearest neighbors in word embedding models are commonly observed to be semantically similar,
                                        but the relations between them can vary greatly. We investigate the extent to which word
                                        embedding models preserve syntactic interchangeability, as reflected by distances between
                                        word vectors, and the effect of hyper-parameters---context window size in particular. We use
                                        part of speech (POS) as a proxy for syntactic interchangeability, as generally speaking,
                                        words with the same POS are syntactically valid in the same contexts. We also investigate
                                        the relationship between interchangeability and similarity as judged by commonly-used word
                                        similarity benchmarks, and correlate the result with the performance of word embedding
                                        models on these benchmarks. Our results will inform future research and applications in the
                                        selection of word embedding model, suggesting a principle for an appropriate selection of
                                        the context window size parameter depending on the use-case.
                                    </i>
                                </div>
                            </div>
                        </li>

                        <li><a id="naacl2019"></a>
                            <div class="row">
                                <div class="col">
                                    <b><i>Content Differences in Syntactic and Semantic Representations</i></b>.
                                    <br>
                                    <b>Daniel Hershcovich</b>, Omri Abend and Ari Rappoport.<br>
                                    <a href='https://naacl2019.org' target="_blank">NAACL 2019</a> (long paper).
                                </div>
                                <div class="col btn-group mr-2 v-center" role="group">
                                    <a href="N19-1047.pdf" class="btn btn-outline-primary btn-sm">pdf</a>
                                    <a href="https://arxiv.org/abs/1903.06494" class="btn btn-outline-primary btn-sm">preprint</a>
                                    <a href="https://www.aclweb.org/anthology/N19-1047" class="btn btn-outline-primary btn-sm">proceedings</a>
                                    <a href="#" class="btn btn-outline-primary btn-sm"
                                       onclick="$('#naacl2019_abstract').slideToggle(); $(this).toggleClass('active'); return false;">abstract</a>
                                    <a href="divergences.bib" class="btn btn-outline-primary btn-sm bib">bib</a>
                                    <a href="N19-1047.Supplementary.pdf" class="btn btn-outline-primary btn-sm">supplementary
                                        material</a>
                                    <a href="divergences_poster.pdf" class="btn btn-outline-primary btn-sm">poster</a>
                                    <a href="https://github.com/danielhers/synsem" class="btn btn-outline-primary btn-sm">code</a>
                                    <a href="https://github.com/UniversalConceptualCognitiveAnnotation/UCCA_English-EWT"
                                       class="btn btn-outline-primary btn-sm">data</a>
                                    <a href="https://github.com/danielhers/phd-papers" class="btn btn-outline-primary btn-sm">source</a>
                                    <a href="ucca/divergences.html" class="btn btn-outline-primary btn-sm">examples</a>
                                </div>
                                <div id="naacl2019_abstract" class="abstract row" style="display:none;">
                                    <i>Syntactic analysis plays an important role in semantic parsing, but this role remains a topic
                                        of ongoing debate. The debate has been constrained by the scarcity of empirical comparative
                                        studies between syntactic and semantic schemes, which hinders the development of parsing
                                        methods informed by the details of target schemes and constructions. We target this gap, and
                                        take Universal Dependencies (UD) and UCCA as a test case. After abstracting away from
                                        differences of convention or formalism, we find that most content divergences can be
                                        ascribed to: (1) UCCA's distinction between a Scene and a non-Scene; (2) UCCA's distinction
                                        between primary relations, secondary ones and participants; (3) different treatment of
                                        multi-word expressions, and (4) different treatment of inter-clause linkage. We further
                                        discuss the long tail of cases where the two schemes take markedly different approaches.
                                        Finally, we show that the proposed comparison methodology can be used for fine-grained
                                        evaluation of UCCA parsing, highlighting both challenges and potential sources for
                                        improvement. The substantial differences between the schemes suggest that semantic parsers
                                        are likely to benefit downstream text understanding applications beyond their syntactic
                                        counterparts.
                                    </i>
                                </div>
                            </div>
                        </li>

                        <li><a id="udst2018"></a>
                            <div class="row">
                                <div class="col">
                                    <b><i>Universal Dependency Parsing with a General Transition-Based DAG Parser</i></b>.
                                    <br>
                                    <b>Daniel Hershcovich</b>, Omri Abend and Ari Rappoport.<br>
                                    <a href='http://universaldependencies.org/conll18' target="_blank">CoNLL 2018 UD Shared Task</a>.
                                </div>
                                <div class="col btn-group mr-2 v-center" role="group">
                                    <a href="K18-2010.pdf" class="btn btn-outline-primary btn-sm">pdf</a>
                                    <a href="https://arxiv.org/abs/1808.09354" class="btn btn-outline-primary btn-sm">preprint</a>
                                    <a href="https://www.aclweb.org/anthology/K18-2010" class="btn btn-outline-primary btn-sm">proceedings</a>
                                    <a href="#" class="btn btn-outline-primary btn-sm"
                                       onclick="$('#udst2018_abstract').slideToggle(); $(this).toggleClass('active'); return false;">abstract</a>
                                    <a href="k18-2010.bib" class="btn btn-outline-primary btn-sm bib">bib</a>
                                    <a href="udst2018_poster.pdf" class="btn btn-outline-primary btn-sm">poster</a>
                                    <a href="https://github.com/CoNLL-UD-2018/HUJI" class="btn btn-outline-primary btn-sm">code</a>
                                    <a href="https://github.com/danielhers/phd-papers" class="btn btn-outline-primary btn-sm">source</a>
                                </div>
                                <div id="udst2018_abstract" class="abstract row" style="display:none;">
                                    <i>This paper presents our experiments with applying TUPA to the CoNLL 2018 UD shared task. TUPA
                                        is a general neural transition-based DAG parser, which we use to present the first
                                        experiments on recovering enhanced dependencies as part of the general parsing task. TUPA
                                        was designed for parsing UCCA, a cross-linguistic semantic annotation scheme, exhibiting
                                        reentrancy, discontinuity and non-terminal nodes. By converting UD trees and graphs to a
                                        UCCA-like DAG format, we train TUPA almost without modification on the UD parsing task. The
                                        generic nature of our approach lends itself naturally to multitask learning.</i>
                                </div>
                            </div>
                        </li>

                        <li><a id="acl2018"></a>
                            <div class="row">
                                <div class="col">
                                    <b><i>Multitask Parsing Across Semantic Representations</i></b>.
                                    <br>
                                    <b>Daniel Hershcovich</b>, Omri Abend and Ari Rappoport.<br>
                                    <a href='http://acl2018.org' target="_blank">ACL 2018</a> (long paper).
                                </div>
                                <div class="col btn-group mr-2 v-center" role="group">
                                    <a href="P18-1035.pdf" class="btn btn-outline-primary btn-sm">pdf</a>
                                    <a href="https://arxiv.org/abs/1805.00287" class="btn btn-outline-primary btn-sm">preprint</a>
                                    <a href="https://www.aclweb.org/anthology/P18-1035" class="btn btn-outline-primary btn-sm">proceedings</a>
                                    <a href="#" class="btn btn-outline-primary btn-sm"
                                       onclick="$('#acl2018_abstract').slideToggle(); $(this).toggleClass('active'); return false;">abstract</a>
                                    <a href="p18-1035.bib" class="btn btn-outline-primary btn-sm bib">bib</a>
                                    <a href="P18-1035.Notes.pdf" class="btn btn-outline-primary btn-sm">supplementary material</a>
                                    <a href="acl2018_poster.pdf" class="btn btn-outline-primary btn-sm">poster</a>
                                    <a href="https://github.com/danielhers/tupa" class="btn btn-outline-primary btn-sm">code</a>
                                    <a href="https://github.com/UniversalConceptualCognitiveAnnotation/UCCA_English-WSJ"
                                       class="btn btn-outline-primary btn-sm">data</a>
                                    <a href="https://github.com/danielhers/phd-papers" class="btn btn-outline-primary btn-sm">source</a>
                                </div>
                                <div id="acl2018_abstract" class="abstract row" style="display:none;">
                                    <i>The ability to consolidate information of different types is at the core of intelligence, and
                                        has tremendous practical value in allowing learning for one task to benefit from
                                        generalizations learned for others. In this paper we tackle the challenging task of
                                        improving semantic parsing performance, taking UCCA parsing as a test case, and AMR, SDP and
                                        Universal Dependencies (UD) parsing as auxiliary tasks. We experiment on three languages,
                                        using a uniform transition-based system and learning architecture for all parsing tasks.
                                        Despite notable conceptual, formal and domain differences, we show that multitask learning
                                        significantly improves UCCA parsing in both in-domain and out-of-domain settings.</i>
                                </div>
                            </div>
                        </li>

                        <li><a id="acl2017"></a>
                            <div class="row">
                                <div class="col">
                                    <b><i>A Transition-Based Directed Acyclic Graph Parser for UCCA</i></b>.
                                    <br>
                                    <b>Daniel Hershcovich</b>, Omri Abend and Ari Rappoport.<br>
                                    <a href='http://acl2017.org' target="_blank">ACL 2017</a> (long paper).
                                    <a href='https://acl2017.wordpress.com/2017/08/03/outstanding-and-best-papers-and-the-decision-process/'
                                       target="_blank"><span style="color:red">Outstanding Paper Award</span></a>.
                                </div>
                                <div class="col btn-group mr-2 v-center" role="group">
                                    <a href="P17-1104.pdf" class="btn btn-outline-primary btn-sm">pdf</a>
                                    <a href="https://arxiv.org/abs/1704.00552" class="btn btn-outline-primary btn-sm">preprint</a>
                                    <a href="https://www.aclweb.org/anthology/P17-1104" class="btn btn-outline-primary btn-sm">proceedings</a>
                                    <a href="#" class="btn btn-outline-primary btn-sm"
                                       onclick="$('#P17-1104_abstract').slideToggle(); $(this).toggleClass('active'); return false;">abstract</a>
                                    <a href="P17-1104.bib" class="btn btn-outline-primary btn-sm bib">bib</a>
                                    <a href="P17-1104_supp.pdf" class="btn btn-outline-primary btn-sm">supplementary material</a>
                                    <a href="acl2017_slides.pdf" class="btn btn-outline-primary btn-sm">slides</a>
                                    <a href="iscol2017_poster.pdf" class="btn btn-outline-primary btn-sm">poster</a>
                                    <a href="https://github.com/huji-nlp/tupa" class="btn btn-outline-primary btn-sm">code</a>
                                    <a href="https://github.com/huji-nlp/tupa-demo" class="btn btn-outline-primary btn-sm">demo</a>
                                    <a href="https://github.com/UniversalConceptualCognitiveAnnotation"
                                       class="btn btn-outline-primary btn-sm">data</a>
                                    <a href="https://github.com/danielhers/phd-papers" class="btn btn-outline-primary btn-sm">source</a>
                                </div>
                                <div id="P17-1104_abstract" class="abstract row" style="display:none;">
                                    <i>We present the first parser for UCCA, a cross-linguistically applicable framework for
                                        semantic
                                        representation, which builds on extensive typological work and supports rapid annotation.
                                        UCCA
                                        poses a challenge for existing parsing techniques, as it exhibits reentrancy (resulting in
                                        DAG
                                        structures), discontinuous structures and non-terminal nodes corresponding to complex
                                        semantic
                                        units. To our knowledge, the conjunction of these formal properties is not supported by any
                                        existing parser. Our transition-based parser, which uses a novel transition set and features
                                        based on bidirectional LSTMs, has value not just for UCCA parsing: its ability to handle
                                        more
                                        general graph structures can inform the development of parsers for other semantic DAG
                                        structures, and in languages that frequently use discontinuous structures.</i>
                                </div>
                            </div>
                        </li>

                        <li>
                            <div class="row">
                                <div class="col">
                                    <b><i>Automatic Claim Negation: Why, How and When</i></b>.
                                    <br>
                                    Yonatan Bilu, <b>Daniel Hershcovich</b>, Noam Slonim.<br>
                                    <a href='http://naacl.org/naacl-hlt-2015/' target="_blank">NAACL HLT 2015</a> (long paper).
                                </div>
                                <div class="col btn-group mr-2 v-center" role="group">
                                    <a href="https://www.aclweb.org/anthology/W15-0511" class="btn btn-outline-primary btn-sm">proceedings</a>
                                    <a href="#" class="btn btn-outline-primary btn-sm"
                                       onclick="$('#negation_abstract').slideToggle(); $(this).toggleClass('active'); return false;">abstract</a>
                                    <a href="W15-0511.bib"
                                       class="btn btn-outline-primary btn-sm bib">bib</a>
                                </div>
                                <div id="negation_abstract" class="abstract" style="display:none;">
                                    <i>The main goal of argumentation mining is to analyze argumentative structures within an
                                        argument-rich document, and reason about their composition. Recently, there is also interest
                                        in
                                        the task of simply detecting claims (sometimes called conclusion) in general documents. In
                                        this
                                        work we ask how this set of detected claims can be augmented further, by adding to it the
                                        negation of each detected claim. This presents two NLP problems: how to automatically negate
                                        a
                                        claim, and when such a negated claim can plausibly be used. We present first steps into
                                        solving
                                        both these problems, using a rule-based approach for the former and a statistical one
                                        towards
                                        the latter.</i>
                                </div>
                            </div>
                        </li>

                        <li>
                            <div class="row">
                                <div class="col">
                                    <b><i>Context Dependent Claim Detection</i></b>.
                                    <br>
                                    Ran Levy, Yonatan Bilu, <b>Daniel Hershcovich</b>, Ehud Aharoni and Noam Slonim.<br>
                                    <a href='http://www.coling-2014.org/' target="_blank">COLING 2014</a> (long paper).
                                </div>
                                <div class="col btn-group mr-2 v-center" role="group">
                                    <a href="http://www.aclweb.org/anthology/C14-1141"
                                       class="btn btn-outline-primary btn-sm">proceedings</a>
                                    <a href="#" class="btn btn-outline-primary btn-sm"
                                       onclick="$('#cdcd_abstract').slideToggle(); $(this).toggleClass('active'); return false;">abstract</a>
                                    <a href="C14-1141.bib" class="btn btn-outline-primary btn-sm bib">bib</a>
                                </div>
                                <div id="cdcd_abstract" class="abstract" style="display:none;">
                                    <i>While discussing a concrete controversial topic, most humans will find it challenging to
                                        swiftly
                                        raise a diverse set of convincing and relevant claims that should set the basis of their
                                        arguments. Here, we formally define the challenging task of automatic claim detection in a
                                        given
                                        context and discuss its associated unique difficulties. Further, we outline a preliminary
                                        solution to this task, and assess its performance over annotated real world data, collected
                                        specifically for that purpose over hundreds of Wikipedia articles. We report promising
                                        results
                                        of a supervised learning approach, which is based on a cascade of classifiers designed to
                                        properly handle the skewed data which is inherent to the defined task. These results
                                        demonstrate
                                        the viability of the introduced task.</i>
                                </div>
                            </div>
                        </li>

                        <li>
                            <div class="row">
                                <div class="col">
                                    <b><i>Claims on Demand–an Initial Demonstration of a System for Automatic Detection and
                                        Polarity Identification of Context Dependent Claims in Massive Corpora</i></b>.
                                    <br>
                                    Ehud Aharoni, Carlos Alzate, Roy Bar-Haim, Yonatan Bilu, Lena Dankin, Iris Eiron, <b>Daniel
                                    Hershcovich</b>, Shay Hummel.<br>
                                    <a href='http://www.coling-2014.org/' target="_blank">COLING 2014</a> (system demonstration).
                                </div>
                                <div class="col btn-group mr-2 v-center" role="group">
                                    <a href="http://www.aclweb.org/anthology/C14-2002"
                                       class="btn btn-outline-primary btn-sm">proceedings</a>
                                    <a href="#" class="btn btn-outline-primary btn-sm"
                                       onclick="$('#acedemo_abstract').slideToggle(); $(this).toggleClass('active'); return false;">abstract</a>
                                    <a href="C14-2002.bib" class="btn btn-outline-primary btn-sm bib">bib</a>
                                </div>
                                <div id="acedemo_abstract" class="abstract" style="display:none;">
                                    <i>While discussing a concrete controversial topic, most humans will find it challenging to
                                        swiftly
                                        raise a diverse set of convincing and relevant claims that should set the basis of their
                                        arguments. Here, we demonstrate the initial capabilities of a system that, given a
                                        controversial
                                        topic, can automatically pinpoint relevant claims in Wikipedia, determine their polarity
                                        with
                                        respect to the given topic, and articulate them per the user's request.</i>
                                </div>
                            </div>
                        </li>

                        <li>
                            <div class="row">
                                <div class="col">
                                    <b><i>A Benchmark Dataset for Automatic Detection of Claims and Evidence in the Context of
                                        Controversial Topics</i></b>.
                                    <br>
                                    Ehud Aharoni, Anatoly Polnarov, Tamar Lavee, <b>Daniel Hershcovich</b>, Ran Levy, Ruty Rinott,
                                    Dan Gutfreund, Noam Slonim.<br>
                                    <a href='https://www.uncg.edu/cmp/ArgMining2014/' target="_blank">Workshop on Argumentation
                                        Mining</a> at <a href='http://acl2014.org/' target="_blank">ACL 2014</a>.
                                </div>
                                <div class="col btn-group mr-2 v-center" role="group">
                                    <a href="http://anthology.aclweb.org/W14-2109"
                                       class="btn btn-outline-primary btn-sm">proceedings</a>
                                    <a href="#" class="btn btn-outline-primary btn-sm"
                                       onclick="$('#acelab_abstract').slideToggle(); $(this).toggleClass('active'); return false;">abstract</a>
                                    <a href="W14-2109.bib" class="btn btn-outline-primary btn-sm bib">bib</a>
                                </div>
                                <div id="acelab_abstract" class="abstract" style="display:none;">
                                    <i>We describe a novel and unique argumentative structure dataset. This corpus consists of data
                                        extracted fro m hundreds of Wikipedia articles using a meticulously monitored manual
                                        annotation
                                        process. The result is 2,683 argument elements, collected in the context of 33 controversial
                                        topics, organized under a simple claim-evidence structure. The obtained data are publicly
                                        available for academic research.</i>
                                </div>
                            </div>
                        </li>

                        <li>
                            <div class="row">
                                <div class="col">
                                    <b><i>Verification of Transactional Memory in POWER8</i></b>.
                                    <br>
                                    Alon Adir, Dave Goodman, <b>Daniel Hershcovich</b>, Oz Hershkovitz, Bryan Hickerson, Karen
                                    Holtz, Wisam Kadry, Anatoly Koyfman, John Ludden, Charles Meissner, Amir Nahir, Randall R Pratt,
                                    Mike Schiffli, Brett St Onge, Brian Thompto, Elena Tsanko, Avi Ziv.<br>
                                    <a href='https://www.ieee.org/conferences_events/conferences/conferencedetails/index.html?Conf_ID=18068'
                                       target="_blank">DAC 2014</a>.
                                </div>
                                <div class="col btn-group mr-2 v-center" role="group">
                                    <a href="http://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=6881385"
                                       class="btn btn-outline-primary btn-sm">pdf</a>
                                    <a href="#" class="btn btn-outline-primary btn-sm"
                                       onclick="$('#power8tx_abstract').slideToggle(); $(this).toggleClass('active'); return false;">abstract</a>
                                    <a href="2593241.bib" class="btn btn-outline-primary btn-sm bib">bib</a>
                                </div>
                                <div id="power8tx_abstract" class="abstract" style="display:none;">
                                    <i>Transactional memory is a promising mechanism for synchronizing concurrent programs that
                                        eliminates locks at the expense of hardware complexity. Transactional memory is a hard
                                        feature
                                        to verify. First, transactions comprise several instructions that must be observed as a
                                        single
                                        global atomic operation. In addition, there are many reasons a transaction can fail. This
                                        results in a high level of non-determinism which must be tamed by the verification
                                        methodology.
                                        This paper describes the innovation that was applied to tools and methodology in pre-silicon
                                        simulation, acceleration and post-silicon in order to verify transactional memory in the IBM
                                        POWER8 processor core.</i>
                                </div>
                            </div>
                        </li>

                      <h3 class="header">Preprints</h3>

                      <li><a id="gpt4v-marvl"></a>
                        <div class="row">
                          <div class="col">
                            <b><i>Exploring Visual Culture Awareness in GPT-4V: A Comprehensive Probing</i></b>.
                            <br>
                            Yong Cao, Wenyan Li, Jiaang Li, Yifei Yuan and <b>Daniel Hershcovich</b>.
                          </div>
                          <div class="col btn-group mr-2 v-center" role="group">
                            <a href="https://arxiv.org/abs/2402.06015" class="btn btn-outline-primary btn-sm">preprint</a>
                            <a href="#" class="btn btn-outline-primary btn-sm"
                                        onclick="$('#gpt4v-marvl_abstract').slideToggle(); $(this).toggleClass('active'); return false;">abstract</a>
                            <a href="gpt4v-marvl.bib" class="btn btn-outline-primary btn-sm bib">bib</a>
                          </div>
                          <div id="gpt4v-marvl_abstract" class="abstract row" style="display:none;">
                            <i>Pretrained large Vision-Language models have drawn considerable interest in recent years due to their remarkable performance. Despite considerable efforts to assess these models from diverse perspectives, the extent of visual cultural awareness in the state-of-the-art GPT-4V model remains unexplored. To tackle this gap, we extensively probed GPT-4V using the MaRVL benchmark dataset, aiming to investigate its capabilities and limitations in visual understanding with a focus on cultural aspects. Specifically, we introduced three visual related tasks, i.e. caption classification, pairwise captioning, and culture tag selection, to systematically delve into fine-grained visual cultural evaluation. Experimental results indicate that GPT-4V excels at identifying cultural concepts but still exhibits weaker performance in low-resource languages, such as Tamil and Swahili. Notably, through human evaluation, GPT-4V proves to be more culturally relevant in image captioning tasks than the original MaRVL human annotations, suggesting a promising solution for future visual cultural benchmark construction.</i>
                          </div>
                        </div>
                      </li>

                      <li><a id="gmr"></a>
                        <div class="row">
                          <div class="col">
                            <b><i>Revisiting Graph Meaning Representations through Decoupling Contextual Representation Learning and Structural Information Propagation</i></b>.
                            <br>
                            Li Zhou, Wenyu Chen, Dingyi Zeng, Hong Qu and <b>Daniel Hershcovich</b>.
                          </div>
                          <div class="col btn-group mr-2 v-center" role="group">
                            <a href="https://arxiv.org/abs/2310.09772" class="btn btn-outline-primary btn-sm">preprint</a>
                            <a href="#" class="btn btn-outline-primary btn-sm"
                                        onclick="$('#gmr_abstract').slideToggle(); $(this).toggleClass('active'); return false;">abstract</a>
                            <a href="gmr.bib" class="btn btn-outline-primary btn-sm bib">bib</a>
                          </div>
                          <div id="gmr_abstract" class="abstract row" style="display:none;">
                            <i>In the field of natural language understanding, the intersection of neural models and graph meaning representations (GMRs) remains a compelling area of research. Despite the growing interest, a critical gap persists in understanding the exact influence of GMRs, particularly concerning relation extraction tasks. Addressing this, we introduce DAGNN-plus, a simple and parameter-efficient neural architecture designed to decouple contextual representation learning from structural information propagation. Coupled with various sequence encoders and GMRs, this architecture provides a foundation for systematic experimentation on two English and two Chinese datasets. Our empirical analysis utilizes four different graph formalisms and nine parsers. The results yield a nuanced understanding of GMRs, showing improvements in three out of the four datasets, particularly favoring English over Chinese due to highly accurate parsers. Interestingly, GMRs appear less effective in literary-domain datasets compared to general-domain datasets. These findings lay the groundwork for better-informed design of GMRs and parsers to improve relation classification, which is expected to tangibly impact the future trajectory of natural language understanding research.</i>
                          </div>
                        </div>
                      </li>

                      <li><a id="cognitive-mr"></a>
                        <div class="row">
                          <div class="col">
                            <b><i>Does injecting linguistic structure into language models lead to better alignment with brain recordings?</i></b>.
                            <br>
                            Mostafa Abdou, Ana Valeria Gonzalez, Mariya Toneva, <b>Daniel Hershcovich</b> and Anders Søgaard.
                          </div>
                          <div class="col btn-group mr-2 v-center" role="group">
                            <a href="https://arxiv.org/abs/2101.12608" class="btn btn-outline-primary btn-sm">preprint</a>
                            <a href="#" class="btn btn-outline-primary btn-sm"
                                        onclick="$('#cognitive-mr_abstract').slideToggle(); $(this).toggleClass('active'); return false;">abstract</a>
                            <a href="cognitive-mr.bib" class="btn btn-outline-primary btn-sm bib">bib</a>
                            <a href="https://github.com/mhany90/Structural_bias_brain"
                               class="btn btn-outline-primary btn-sm">code</a>
                          </div>
                          <div id="cognitive-mr_abstract" class="abstract row" style="display:none;">
                            <i>Neuroscientists evaluate deep neural networks for natural language processing as possible candidate models for how language is processed in the brain. These models are often trained without explicit linguistic supervision, but have been shown to learn some linguistic structure in the absence of such supervision (Manning et al., 2020), potentially questioning the relevance of symbolic linguistic theories in modeling such cognitive processes (Warstadt and Bowman, 2020). We evaluate across two fMRI datasets whether language models align better with brain recordings, if their attention is biased by annotations from syntactic or semantic formalisms. Using structure from dependency or minimal recursion semantic annotations, we find alignments improve significantly for one of the datasets. For another dataset, we see more mixed results. We present an extensive analysis of these results. Our proposed approach enables the evaluation of more targeted hypotheses about the composition of meaning in the brain, expanding the range of possible scientific inferences a neuroscientist could make, and opens up new opportunities for cross-pollination between computational neuroscience and linguistics.</i>
                          </div>
                        </div>
                      </li>

                        <h3 class="header">Dissertations</h3>

                        <li><a id="thesis"></a>
                            <div class="row">
                                <div class="col">
                                    <b><i>Universal Semantic Parsing with Neural Networks</i></b>.
                                    <br>
                                    <b>Daniel Hershcovich</b>.<br>
                                    <a href="https://huji-primo.hosted.exlibrisgroup.com/permalink/f/att40d/972HUJI_ALMA21257777500003701">PhD
                                        dissertation, Hebrew University of Jerusalem</a>, 2019.
                                </div>
                                <div class="col btn-group mr-2 v-center" role="group">
                                    <a href="thesis.pdf"
                                       class="btn btn-outline-primary btn-sm">pdf</a>
                                    <a href="#" class="btn btn-outline-primary btn-sm"
                                       onclick="$('#thesis_abstract').slideToggle(); $(this).toggleClass('active'); return false;">abstract</a>
                                    <a href="thesis.bib" class="btn btn-outline-primary btn-sm bib">bib</a>
                                    <a href="thesis_slides.pdf" class="btn btn-outline-primary btn-sm">slides</a>
                                    <a href="https://github.com/danielhers/phd" class="btn btn-outline-primary btn-sm">source</a>
                                </div>
                                <div id="thesis_abstract" class="abstract row" style="display:none;">
                                    <i>A major scientific effort is dedicated to natural language understanding, which aims to be
                                        able to comprehend text, reason about it, and act upon it in an intelligent way. While
                                        specific use-cases or benchmarks can be solved with relatively simple systems, which either
                                        ignore word order ("bag-of-words" models) or treat it as a simple linear structure (such as
                                        the popular sequence-to-sequence framework allowing neural networks to learn tasks in an
                                        end-to-end fashion), understanding human language in general requires a hierarchical
                                        representation of meaning. Constructing this representation from text has been the goal of
                                        an extensive line of work in semantic parsing. While many semantic representation schemes
                                        have been proposed, they share many of their basic distinctions, such as between predicates
                                        (relations, states and events) and arguments (participants). This thesis focuses on a
                                        particular semantic representation scheme called Universal Conceptual Cognitive Annotation
                                        (UCCA), whose main design principles are support for all major linguistic semantic
                                        phenomena, cross-linguistic applicability, stability across translations, ease of annotation
                                        (even by those who are not experts in linguistics), and a modular architecture supporting
                                        multiple layers of semantic annotation. A fully automatic parser is presented, and evaluated
                                        on multiple languages (English, French and German). The parser, titled "TUPA"
                                        (transition-based UCCA parser), is able to learn very general graph structures: directed
                                        acyclic graphs over token sequences with non-terminal nodes for complex units, where these
                                        may cover discontinuous terminal yields. This general class of graphs covers the structures
                                        annotated in UCCA, as well as other representation schemes. TUPA is implemented as a
                                        transition-based parser, whose transition system supports these structural properties. Its
                                        transition classifier is a neural network equipped with a bidirectional long short-term
                                        memory (BiLSTM) module for calculating feature representations for the input. In an
                                        extensive comparison to conversion-based methods, as well as other classifier
                                        implementations, TUPA is shown to outperform all baselines in the task of UCCA parsing in
                                        both in-domain and out-of-domain settings in three languages. The parser is subsequently
                                        applied to two other semantic representation schemes, DM and AMR, and to syntactic
                                        dependencies in the Universal Dependencies (UD) scheme. This demonstrates that the flexible
                                        parser is usable not just for UCCA parsing. Furthermore, training TUPA in a multitask
                                        setting on all of these schemes improves its UCCA parsing accuracy, by effectively learning
                                        generalizations across the different representations: a shared model is thus able to apply
                                        semantic distinctions in one task, which have been learned for another. Finally, in an
                                        empirical comparison of the content of semantic and syntactic representations, we discover
                                        several aspects of divergence, i.e., differences in the content captured by these schemes.
                                        These have profound impact on the potential contribution of syntax to semantic parsing, and
                                        on the usefulness of each of the approaches for semantic tasks in natural language
                                        processing. I see semantic parsing as a means for computers to learn language. While
                                        different representations focus on different distinctions and do so with formally different
                                        structures, they share an overall goal, which is to support natural language processing
                                        applications, such as classifying text into categories, tagging it for linguistic
                                        properties, performing inference and reasoning, and generating new text according to some
                                        constraints (e.g., machine translation). The combined datasets annotated in every
                                        representation are an invaluable resource, which, used effectively, can greatly boost our
                                        achievements in language understanding and processing.
                                    </i>
                                </div>
                            </div>
                        </li>

                        <h3 class="header">Tutorials</h3>
                        <li><a id="ucca_tutorial_coling2020"></a>
                            <div class="row">
                                <div class="col">
                                    <b><i>Cross-lingual Semantic Representation for NLP with UCCA</i></b>.
                                    <br>Omri Abend, Dotan Dvir, <b>Daniel Hershcovich</b>, Jakob Prange and Nathan Schneider.<br>
                                    <a href='https://coling2020.org/pages/tutorials' target="_blank">COLING 2020 tutorial</a>.
                                </div>
                                <div class="col btn-group mr-2 v-center" role="group">
                                    <a href="https://www.aclweb.org/anthology/2020.coling-tutorials.1/"
                                       class="btn btn-outline-primary btn-sm">proceedings</a>
                                    <a href="#" class="btn btn-outline-primary btn-sm"
                                       onclick="$('#ucca_tutorial_coling2020_abstract').slideToggle(); $(this).toggleClass('active'); return false;">abstract</a>
                                    <a href="2020.coling-tutorials.1.bib" class="btn btn-outline-primary btn-sm bib">bib</a>
                                    <a href="https://github.com/UniversalConceptualCognitiveAnnotation/tutorial" class="btn btn-outline-primary btn-sm">slides</a>
                                    <a href="https://github.com/UniversalConceptualCognitiveAnnotation/tutorial/releases/tag/coling2020" class="btn btn-outline-primary btn-sm">videos</a>
                                </div>
                                <div id="ucca_tutorial_coling2020_abstract" class="abstract row" style="display:none;">
                                    <i>This is an introductory tutorial to UCCA (Universal Conceptual Cognitive Annotation), a cross-linguistically applicable framework for semantic representation, with corpora annotated in English, German and French, and ongoing annotation in Russian and Hebrew. UCCA builds on extensive typological work and supports rapid annotation. The tutorial will provide a detailed introduction to the UCCA annotation guidelines, design philosophy and the available resources; and a comparison to other meaning representations. It will also survey the existing parsing work, including the findings of three recent shared tasks, in SemEval and CoNLL, that addressed UCCA parsing. Finally, the tutorial will present recent applications and extensions to the scheme, demonstrating its value for natural language processing in a range of languages and domains.
                                    </i>
                                </div>
                            </div>
                        </li>

                    </ul>
                </div>
            </div>

            <div class="card">
                <h2 class="header card-header" style="display: inline">
                    <button id="talks" class="btn btn-link" data-toggle="collapse" data-target="#talkList" aria-expanded="true" aria-controls="talkList">Talks</button>
                </h2>
                <div id="talkList" class="talks collapse show" data-parent="#accordion">
                    <ul class="card-body">
                      <li>
                            <div class="row">
                                <div class="col">
                                  <i>Cultural Understanding and Adaptation with AI</i>
                                  <br>
                                  Linguistics and English Language Seminar, University of Manchester (02/2024).
                                </div>
                                <div class="col btn-group mr-2 v-center" role="group">
                                    <a href="#" class="btn btn-outline-primary btn-sm"
                                       onclick="$('#20240228_manchester_abstract').slideToggle(); $(this).toggleClass('active'); return false;">abstract</a>
                                    <a href="20240228_manchester.pptx" class="btn btn-outline-primary btn-sm">slides pptx</a>
                                </div>
                                <div id="20240228_manchester_abstract" class="abstract row" style="display:none;">
                                    <i>In my talk, I address the critical intersection of language and culture within Natural Language Processing (NLP), proposing a novel framework aimed at expanding NLP's focus to not only encompass linguistic diversity but also the nuanced differences dictated by culture. This initiative is crucial for crafting NLP systems that truly reflect and cater to the global mosaic of users, considering how deeply cultural contexts influence language use and content preferences. Through a series of experimental studies and analyses, my research underscores the significant impact of cultural integration into language technologies, ranging from dialogue generation to content adaptation and the evaluation of biases in existing models. By exploring and applying both established and innovative methods for incorporating cultural insights, this work aspires to pioneer more inclusive, respectful, and culturally aware NLP systems, highlighting the ethical and technical challenges involved in bridging the gap between language and culture to better serve the diverse global community.</i>
                                </div>
                            </div>
                      </li>
                      <li>
                            <div class="row">
                                <div class="col">
                                  <i>Cultural Awareness and Adaptation with AI</i>
                                  <br>
                                  QualiTech Research Seminar, IBM Research Haifa (02/2024).
                                </div>
                                <div class="col btn-group mr-2 v-center" role="group">
                                    <a href="#" class="btn btn-outline-primary btn-sm"
                                       onclick="$('#20240219_ibm_abstract').slideToggle(); $(this).toggleClass('active'); return false;">abstract</a>
                                    <a href="20240219_ibm.pptx" class="btn btn-outline-primary btn-sm">slides pptx</a>
                                </div>
                                <div id="20240219_ibm_abstract" class="abstract row" style="display:none;">
                                    <i>This talk explores the intersection of AI and culture, shedding light on the imperative of integrating cultural understanding into Natural Language Processing (NLP). I will discuss empirical assessment of cultural alignment of Large Language Models (LLMs) as well as my two main research agendas: the development of culturally adaptive LLMs and the cross-cultural adaptation of linguistic content. By integrating cultural insights into LLMs, I aim to create technologies that are not only linguistically diverse but also culturally informed, allowing us to better bridge between people from different cultures.</i>
                                </div>
                            </div>
                      </li>
                      <li>
                            <div class="row">
                                <div class="col">
                                  <i>Assessing Knowledge of Cultural Values in ChatGPT​</i>
                                  <br>
                                  Kulturministeriets Tech-Kontor, <a href="https://kum.dk/english/">Ministry of Culture, Denmark</a> (09/2023).
                                </div>
                                <div class="col btn-group mr-2 v-center" role="group">
                                    <a href="20230908_kulturministeriet.pptx" class="btn btn-outline-primary btn-sm">slides pptx</a>
                                </div>
                            </div>
                      </li>
                      <li>
                            <div class="row">
                                <div class="col">
                                  <i>Recipe Adaptation with Language Models</i>
                                  <br>
                                  <a href="https://food.ku.dk/english/">Department of Food Science, University of Copenhagen</a> (06/2023).
                                </div>
                                <div class="col btn-group mr-2 v-center" role="group">
                                    <a href="#" class="btn btn-outline-primary btn-sm"
                                       onclick="$('#food_20230614_abstract').slideToggle(); $(this).toggleClass('active'); return false;">abstract</a>
                                    <a href="food_20230614.pdf" class="btn btn-outline-primary btn-sm">slides pdf</a>
                                    <a href="food_20230614.pptx" class="btn btn-outline-primary btn-sm">slides pptx</a>
                                </div>
                                <div id="food_20230614_abstract" class="abstract row" style="display:none;">
                                    <i>As people explore cuisines from different cultures, it becomes important to adapt recipes to cater to different ingredient availability and preparation methods. In addition, individuals with dietary restrictions, due to health, religious or other reasons, require customized recipe adaptations. In ongoing research, my colleagues and I explore the application of large language models such as ChatGPT to address these issues, automatically adapting recipes according to various constraints. In one project, we use them to generate coherent recipes while adapting to cultural differences. In another, we focus on the challenge of adapting non-vegan recipes to vegan ones while preserving the flavor, cultural style, and aesthetic of the dish. In this presentation, I will highlight the challenges this task raises, as high-quality adaptations require more than simple phrase substitutions – for example, the models are required to infer necessary modifications to preparation methods or introduce creative ingredient substitutions that are both appealing and interesting. This opens the way for a whole new field in Artificial Intelligence, bridging Natural Language Processing and Food Science.</i>
                                </div>
                            </div>
                      </li>
                      <li>
                            <div class="row">
                                <div class="col">
                                  <i>AI 101 – on what ChatGPT is(n’t). Large Language Models and their Potential and Limitations for Language Learning</i>
                                  <br>
                                  <a href="https://cip.ku.dk/english/events/coming_events/baat-bot---ai-coming-through/">Forum on AI in foreign language programmes</a>, University of Copenhagen (05/2023).
                                </div>
                                <div class="col btn-group mr-2 v-center" role="group">
                                    <a href="cip_20230515.pdf" class="btn btn-outline-primary btn-sm">slides pdf</a>
                                    <a href="cip_20230515.pptx" class="btn btn-outline-primary btn-sm">slides pptx</a>
                                </div>
                            </div>
                      </li>
                      <li>
                            <div class="row">
                                <div class="col">
                                  <i>Cultural Adaptation with and of Language Models</i>
                                  <br>
                                  Invited talk at the <a href="https://2023.eacl.org/">EACL 2023</a> workshop <a href="https://sites.google.com/view/c3nlp/home">Cross-Cultural Considerations in NLP</a> (05/2023).
                                </div>
                                <div class="col btn-group mr-2 v-center" role="group">
                                    <a href="#" class="btn btn-outline-primary btn-sm"
                                       onclick="$('#c3nlp_2023_abstract').slideToggle(); $(this).toggleClass('active'); return false;">abstract</a>
                                    <a href="c3nlp_2023.pdf" class="btn btn-outline-primary btn-sm">slides pdf</a>
                                    <a href="c3nlp_2023.pptx" class="btn btn-outline-primary btn-sm">slides pptx</a>
                                </div>
                                <div id="c3nlp_2023_abstract" class="abstract row" style="display:none;">
                                    <i>Large language models provide a unique opportunity to adapt content to the user's culture. However, this raises the risk of perpetuating cultural biases and homogenizing cultural diversity, as many of these models are currently centralized and trained on a limited set of languages and cultures. In this talk, I will emphasize the importance of adapting language models to various cultures to promote linguistic and cultural diversity. This involves not only adapting existing language models but also creating new models and resources that incorporate culturally diverse knowledge and perspectives. This approach will require collaboration and exchange of knowledge across linguistic and cultural boundaries, and has the potential to support intercultural cross-fertilization in various fields, such as literature, anthropology, and beyond. By adapting language models to reflect cultural diversity, we can enable more equitable access to information and foster greater intercultural understanding.</i>
                                </div>
                            </div>
                      </li>
                        <li>
                            <div class="row">
                                <div class="col">
                                  <i>Finding Meaning in Data across Languages</i>
                                  <br>
                                  <a href="https://cst.ku.dk/kalender/sprogteknologisk-konference-2022/">Sprogteknologisk Konference 2022</a>, <a href="https://cst.ku.dk/english">Centre for Language Technology, University of Copenhagen</a> (11/2022)
                                </div>
                                <div class="col btn-group mr-2 v-center" role="group">
                                    <a href="#" class="btn btn-outline-primary btn-sm"
                                       onclick="$('#sprogteknologisk-konference_2022_abstract').slideToggle(); $(this).toggleClass('active'); return false;">abstract</a>
                                    <a href="sprogteknologisk-konference_20221130.pdf" class="btn btn-outline-primary btn-sm">slides pdf</a>
                                    <a href="sprogteknologisk-konference_20221130.pptx" class="btn btn-outline-primary btn-sm">slides pptx</a>
                                </div>
                                <div id="sprogteknologisk-konference_2022_abstract" class="abstract row" style="display:none;">
                                    <i>What is common to complex database queries and beautiful artistic masterpieces? Artificial intelligence can generate both from simple English prompts, and large language models have been instrumental in these achievements. I will show how we can understand the patterns such models find in data using systematic meaning representations, and how to create these representations so we can adapt language models across languages and cultures.</i>
                                </div>
                            </div>
                        </li>
                        <li>
                            <div class="row">
                              <div class="col">
                                <i>Natural Language Processing for Sustainable Diets</i>
                                <br>
                                <a href="https://cape.ku.dk/kalender/the-taste-of-change/">Research Seminar</a> at the <a href="https://cape.ku.dk/">Center for Applied Ecological Thinking (CApE). University of Copenhagen</a> (11/2022)
                              </div>
                              <div class="col btn-group mr-2 v-center" role="group">
                                <a href="cape_20221102.pdf" class="btn btn-outline-primary btn-sm">slides pdf</a>
                              </div>
                            </div>
                        </li>
                        <li>
                          <div class="row">
                            <div class="col">
                              <i>Generalization and Representation in Multilingual Semantic Parsing</i>
                              <br>
                              <a href="http://mousse-project.org/events/event-a5f3r5.html">Workshop on Ten Years of BabelNet and Multilingual Neurosymbolic Natural Language Understanding</a>, <a href="http://nlp.uniroma1.it/">Sapienza University of Rome</a> (07/2022)
                            </div>
                            <div class="col btn-group mr-2 v-center" role="group">
                              <a href="mousse_20220705.pdf" class="btn btn-outline-primary btn-sm">slides pdf</a>
                              <a href="https://youtu.be/Qu0UU3XvKkg?t=2302" class="btn btn-outline-primary btn-sm">recording</a>
                            </div>
                          </div>
                        </li>
                        <li>
                          <div class="row">
                            <div class="col">
                              <i>Argument Mining for Green Nutrition</i>
                              <br>
                              <a href="https://sodas.ku.dk/research/sodas-climate-social-data-science-towards-a-green-transition">SODAS-Climate meeting, University of Copenhagen</a> (05/2022)
                            </div>
                            <div class="col btn-group mr-2 v-center" role="group">
                              <a href="sodas_20220511.pdf" class="btn btn-outline-primary btn-sm">slides pdf</a>
                            </div>
                          </div>
                        </li>
                        <li>
                          <div class="row">
                            <div class="col">
                              <i>Challenges and Strategies in Cross-Cultural NLP</i>
                              <br>
                              <a href="https://liir-kuleuven.github.io/journal-club/reading/">LIIR Journal Club, KU Leuven</a> (04/2022)
                            </div>
                            <div class="col btn-group mr-2 v-center" role="group">
                              <a href="kuleuven_20220421.pdf" class="btn btn-outline-primary btn-sm">slides pdf</a>
                            </div>
                          </div>
                        </li>
                        <li>
                          <div class="row">
                            <div class="col">
                              <i>Cultural and Environmental Considerations in Natural Language Processing</i>
                              <br>
                              Seminar at <a href="https://is-web.hevra.haifa.ac.il/">University of Haifa, Department of Information Systems</a> (03/2022)
                            </div>
                            <div class="col btn-group mr-2 v-center" role="group">
                              <a href="haifa_20220330.pdf" class="btn btn-outline-primary btn-sm">slides pdf</a>
                            </div>
                          </div>
                        </li>
                        <li>
                          <div class="row">
                            <div class="col">
                              <i>Challenges and Strategies in Cross-Cultural NLP</i>
                              <br>
                              NLP Workshop, <a href="https://nlp.itu.dk/">IT University of Copenhagen</a> (03/2022)
                            </div>
                            <div class="col btn-group mr-2 v-center" role="group">
                              <a href="itu_20220315.pdf" class="btn btn-outline-primary btn-sm">slides pdf</a>
                            </div>
                          </div>
                        </li>
                        <li>
                          <div class="row">
                            <div class="col">
                              <i>Meaning Representation and Parsing in Natural Language Processing</i>
                              <br>
                              <a href="https://ifro.ku.dk/english/events/2021/seminar-4may2021/">Seminar</a> at the
                              Department of Food and Resource Economics, University of Copenhagen (05/2021)
                            </div>
                            <div class="col btn-group mr-2 v-center" role="group">
                              <a href="ifro_20210504.pdf" class="btn btn-outline-primary btn-sm">slides pdf</a>
                            </div>
                          </div>
                        </li>
                        <li>
                          <div class="row">
                            <div class="col">
                              <i>Meaning Representation Parsing</i>
                              <br>
                              <a href="https://di.ku.dk/begivenhedsmappe/begivenheder-2020/diku-bits-daniel-hershcovich/">DIKU Bits</a>,
                              Department of Computer Science, University of Copenhagen (02/2020)
                            </div>
                            <div class="col btn-group mr-2 v-center" role="group">
                              <a href="dikubits_20200218.pdf" class="btn btn-outline-primary btn-sm">slides pdf</a>
                            </div>
                          </div>
                        </li>
                        <li>
                          <div class="row">
                            <div class="col">
                              <i>Universal Meaning Representation Parsing</i>
                              <br>
                              <a href="https://www.lingfil.uu.se/calendar/event/?eventId=49730">Seminar in Computational Linguistics</a>,
                              Department of Linguistics and Philology, Uppsala University (11/2019)
                            </div>
                            <div class="col btn-group mr-2 v-center" role="group">
                              <a href="uppsala_20191129.pdf" class="btn btn-outline-primary btn-sm">slides pdf</a>
                            </div>
                          </div>
                        </li>
                        <li>
                          <div class="row">
                            <div class="col">
                              <i>Universal Semantic Parsing with Neural Networks</i>
                              <br>
                              Georgetown University (06/2019)
                            </div>
                            <div class="col btn-group mr-2 v-center" role="group">
                              <a href="georgetown_20190611.pdf" class="btn btn-outline-primary btn-sm">slides pdf</a>
                            </div>
                          </div>
                        </li>
                        <li>
                          <div class="row">
                            <div class="col">
                              <i>A Transition-Based Directed Acyclic Graph Parser for Universal Conceptual Cognitive Annotation</i>
                              <br>
                              Tel Aviv University, NLP Seminar (01/2018)
                            </div>
                            <div class="col btn-group mr-2 v-center" role="group">
                              <a href="tau2018_slides.pdf" class="btn btn-outline-primary btn-sm">slides pdf</a>
                            </div>
                          </div>
                        </li>
                        <li>
                          <div class="row">
                            <div class="col">
                              <i>A Transition-Based Directed Acyclic Graph Parser for Universal Conceptual Cognitive Annotation</i>
                              <br>
                              University of Washington, The Paul G. Allen Center for Computer Science &amp; Engineering (07/2017)
                            </div>
                            <div class="col btn-group mr-2 v-center" role="group">
                              <a href="uw2017_slides.pdf" class="btn btn-outline-primary btn-sm">slides pdf</a>
                            </div>
                          </div>
                        </li>
                        <li>
                          <div class="row">
                            <div class="col">
                              <i>A Transition-Based Directed Acyclic Graph Parser for Universal Conceptual Cognitive Annotation</i>
                              <br>
                              Technion, Computational Data Science Seminar (06/2017)
                            </div>
                            <div class="col btn-group mr-2 v-center" role="group">
                              <a href="technion2017_slides.pdf" class="btn btn-outline-primary btn-sm">slides pdf</a>
                            </div>
                          </div>
                        </li>
                        <li>
                          <div class="row">
                            <div class="col">
                              <i>Broad-Coverage Transition-Based UCCA Parsing</i>
                              <br>
                              Hebrew University, <a href='http://www.cs.huji.ac.il/news-and-events/seminars/learning-club'
                                                    target="_blank">Computer Science Learning Club</a> (11/2016)
                            </div>
                            <div class="col btn-group mr-2 v-center" role="group">
                              <a href="learningclub2016_slides.pdf" class="btn btn-outline-primary btn-sm">slides pdf</a>
                            </div>
                          </div>
                        </li>
                        <li>
                          <div class="row">
                            <div class="col">
                              <i>Broad-Coverage Semantic Parsing: A Transition-Based Approach</i>
                              <br>
                              The Israeli Seminar on Computational Linguistics, <a
                                                                                    href='https://tce.technion.ac.il/events/iscol-2016/' target="_blank">ISCOL 2016</a> (05/2016)
                            </div>
                            <div class="col btn-group mr-2 v-center" role="group">
                              <a href="iscol2016_slides.pdf" class="btn btn-outline-primary btn-sm">slides pdf</a>
                            </div>
                          </div>
                        </li>
                    </ul>
                </div>
            </div>

            <div class="card">
              <h2 class="header card-header" style="display: inline">
                <button id="teaching" class="btn btn-link" data-toggle="collapse" data-target="#teachingList" aria-expanded="true" aria-controls="teachingList">Teaching</button>
              </h2>
              <div id="teachingList" class="teaching collapse show" data-parent="#accordion">
                <div class="card-body">
                  <h3 class="header">University of Copenhagen</h3>
                  <ul>
                    <li>
                      <div class="row">
                        <div class="col">
                          <a href="https://continuing-education.ku.dk/artificial-intelligence/climate-friendly-ai/">Copenhagen Summer University course on Climate Friendly AI</a> 
                      </div>
                    </li>
                    <li>
                      <div class="row">
                        <div class="col">
                          Natural Language Processing (teacher and course coordinator):
                          <a href="https://kurser.ku.dk/course/ndak18000u/2019-2020">2019</a>,
                          <a href="https://kurser.ku.dk/course/ndak18000u/2020-2021">2020</a>,
                          <a href="https://kurser.ku.dk/course/ndak18000u/2021-2022">2021</a>,
                          <a href="https://kurser.ku.dk/course/ndak18000u/2022-2023">2022</a>,
                          <a href="https://kurser.ku.dk/course/ndak18000u/2023-2024">2023</a>
                        </div>
                        <div class="col btn-group mr-2 v-center" role="group">
                          <a href="https://github.com/coastalcph/nlp-course"
                             class="btn btn-outline-primary btn-sm">materials</a>
                        </div>
                      </div>
                    </li>
                    <li>
                      <div class="row">
                        <div class="col">
                          Introduction to Data Science (teacher and course coordinator):
                          <a href="https://kurser.ku.dk/course/ndak16003u/2022-2023">2023</a>,
                          <a href="https://kurser.ku.dk/course/ndak16003u/2023-2024">2024</a>
                        </div>
                      </div>
                    </li>
                    <li>
                      <div class="row">
                        <div class="col">
                          Introduction to Machine Learning (teacher):
                          <a href="https://kurser.ku.dk/course/ndab20000u/2020-2021">2020</a>
                        </div>
                      </div>
                    </li>
                  </ul>

                  <h3 class="header">Hebrew University of Jerusalem</h3>
                  <ul>
                    <li>
                      <div class="row">
                        <div class="col">
                          Human Language from a Computational Perspective (TA):
                          <a href="http://moodle.huji.ac.il/hu14/course/view.php?id=67127">2014</a>,
                          <a href="http://moodle.huji.ac.il/hu15/course/view.php?id=67127">2015</a>,
                          <a href="http://moodle2.cs.huji.ac.il/nu16/course/view.php?id=67127">2016</a>,
                          <a href="https://moodle2.cs.huji.ac.il/nu17/course/view.php?id=67127">2017</a>
                        </div>
                        <div class="col btn-group mr-2 v-center" role="group">
                          <a href="https://github.com/danielhers/avnei_pina"
                             class="btn btn-outline-primary btn-sm">materials</a>
                        </div>
                      </div>
                    </li>
                  </ul>
                </div>
              </div>
            </div>

            <div class="card">
              <h2 class="header card-header" style="display: inline">
                <button id="community" class="btn btn-link" data-toggle="collapse" data-target="#communityList" aria-expanded="true" aria-controls="communityList">Service</button>
              </h2>
              <div id="communityList" class="community collapse show" data-parent="#accordion">
                <ul class="card-body">
                  <li><a href="https://2022.emnlp.org/">EMNLP 2022</a> Workshop Co-chair.</li>
                  <li><a href="http://wiki.nlpl.eu/Community/training">HPLT & NLPL Winter School 2023</a> Program Co-chair.</li>
                  <li>Co-organized <a href="iscol2017">ISCOL 2017</a>,
                    the annual meeting of the Israeli seminar on computational linguistics,
                    as well as <a href="https://competitions.codalab.org/competitions/19160">
                      SemEval 2019 Task 1 on Cross-lingual Semantic Parsing with UCCA</a>,
                    and the <a href="http://mrp.nlpl.eu">
                      CoNLL 2019 and 2020 shared tasks on Cross-Framework Meaning Representation Parsing</a>.
                  </li>
                  <li>Co-presented a tutorial on <a href="https://www.aclweb.org/anthology/2020.coling-tutorials.1/">
                      Cross-lingual Semantic Representation for NLP with UCCA</a>
                    at <a href="https://coling2020.org/">COLING 2020</a>.
                  </li>
                  <li>Guest editor for the <a href="https://www.springer.com/journal/13218/updates/18109720">
                      Künstliche Intelligenz Special Issue on NLP and Semantics</a>.
                  </li>
                  <li>Area chair for <a href="https://2021.naacl.org/">NAACL-HLT 2021</a>,
                    <a href="https://2021.aclweb.org/">ACL-IJCNLP 2021</a> and <a href="https://sites.google.com/view/starsem2021">*SEM 2021</a>.
                  </li>
                  <li>Action editor for ACL Rolling Review since <a href="https://openreview.net/group?id=aclweb.org/ACL/ARR/2021/October">October 2021</a>.</li>
                  <li>Reviewer for
                    ACL (<a href="http://acl2015.org/">2015</a>,
                    <a href="http://mirror.aclweb.org/acl2016/">2016</a>,
                    <a href="http://acl2017.org/">2017</a>,
                    <a href="http://www.acl2019.org">2019</a>,
                    <a href="https://acl2020.org/">2020</a>,
                    <a href="https://2022.aclweb.org/">2022</a>,
                    <a href="https://2023.aclweb.org/">2023</a>),
                    EMNLP (2017, <a href="https://emnlp2018.org/">2018</a>: <a
                                    href="https://www.aclweb.org/anthology/D18-1000.pdf">Best Reviewer Award</a>, <a
                                    href="https://www.emnlp-ijcnlp2019.org/">2019</a>, <a
                                    href="https://2020.emnlp.org/">2020</a>: <a
                                    href="https://www.aclweb.org/anthology/2020.emnlp-main.0.pdf">Outstanding Reviewer</a>, <a
                                    href="https://2023.emnlp.org/">2023</a>),
                                  <a href="http://ijcnlp2017.org">IJCNLP 2017</a>,
                                  COLING (<a href="http://coling2018.org/">2018</a>, <a href="https://coling2020.org/">2020</a>, <a href="http://coling2022.org/">2022</a>),
                                  <a href="https://naacl2019.org/">NAACL-HLT 2019</a>,
                                  *SEM (<a href="https://starsem.org/2019/">2019</a>,
                                  <a href="https://sites.google.com/view/starsem2020">2020</a>,
                                  <a href="https://sites.google.com/view/starsem2022">2022</a>),
                                  DMR (<a href="https://www.cs.brandeis.edu/~clp/dmr/">2019</a>, <a
                                          href="https://www.cs.brandeis.edu/~clp/dmr2020/">2020</a>,
                      <a href="https://dmr2023.github.io/">2023</a>),
                                        NoDaLiDa (<a href="https://nodalida2019.org/">2019</a>,
                                        <a href="https://nodalida2021.github.io/">2021</a>,
                                        <a href="https://nodalida2023.fo">2023</a>),
                                        <a href="https://www.eurnlp.org/">EurNLP 2019</a>,
                                        <a href="https://aaai.org/Conferences/AAAI-20/">AAAI 2020</a>,
                                        <a href="https://www.ijcai20.org/">IJCAI 2020</a>,
                                        IWPT (<a href="https://iwpt20.sigparse.org/">2020</a>, <a href="https://iwpt21.sigparse.org/">2021</a>)
                                        <a href="http://aacl2020.org/">AACL-IJCNLP 2020</a>,
                                        CoNLL (<a href="https://www.conll.org/2020">2020</a>, <a href="https://conll.org/2022">2022</a>, <a href="https://www.conll.org/2023">2023</a>),
                                        <a href="https://tlt2020.phil.hhu.de/">TLT 2020</a>,
                                        <a href="https://2021.eacl.org/">EACL 2021</a>
                                        and the journals
                                        <a href="http://cljournal.org/">Computational Linguistics</a>,
                                        <a href="https://www.journals.elsevier.com/computer-speech-and-language">Computer Speech & Language</a> and
                                        <a href="https://www.springer.com/journal/10579">Language Resources and Evaluation</a>.
                  </li>
                </ul>
              </div>
            </div>

            <div class="card">
              <h2 class="header card-header" style="display: inline">
                <button id="media" class="btn btn-link" data-toggle="collapse" data-target="#mediaList" aria-expanded="true" aria-controls="miscList">Media</button>
              </h2>
              <div id="mediaList" class="media collapse show" data-parent="#accordion">
                <ul class="card-body">
                  <li><a href="https://m.youtube.com/watch?v=HdPcTtq5LMA"><i>Visions of a Connected Future</i>, roundtable on AI</a>, <a href="https://cifs.dk/">Copenhagen Institute for Futures Studies</a> (08/01/2023)</li>
                  <li><a href="#sustainabletweets">Sustainable Diet Arguments on Twitter:</a>
                  <ul>
                    <li><a href="https://klimamonitor.dk/nyheder/art9168139/Kunstig-intelligens-udvinder-god-kommunikation-om-klimavenlige-madvalg-af-30.000-tweets">Kunstig intelligens 'udvinder' god kommunikation om klimavenlige madvalg af 30.000 tweets</a>, Klimamonitor (13/01/2023)</li>
                    <li><a href="https://csr.dk/forskere-bruger-ai-til-f%C3%A5-viden-om-b%C3%A6redygtig-kost-p%C3%A5-twitter">Forskere bruger AI til at få viden om bæredygtig kost på Twitter</a>, CSR.dk (17/01/2023)</li>
                    <li><a href="https://science.ku.dk/english/press/news/2023/nuggets-mined-from-thousands-of-tweets-can-persuade-us-to-eat-more-climate-friendly/">Nuggets mined from thousands of tweets can persuade us to eat more climate-friendly</a>, UCPH SCIENCE News (30/01/2023)</li>
                    <li><a href="https://videnskab.dk/teknologi/kunstig-intelligens-kan-maale-folkestemningen-nyttigt-for-firmaer-og-politikere-men-kan-det-misbruges/">Kunstig intelligens kan måle folkestemningen: Nyttigt for firmaer og politikere, men kan det misbruges?</a>, Videnskab.dk (30/01/2023)</li>
                    <li><a href="https://indiaeducationdiary.in/university-of-copenhagen-researchers-demonstrate-ais-role-in-sustainable-food/">University Of Copenhagen Researchers Demonstrate AI’s Role In Sustainable Food</a>, India Education Diary (11/02/2023)</li>
                    <li><a href="https://pov.international/argument-mining-koebenhavns-universitet/">‘Argument mining’: Guldkorn fra tusindvis af tweets kan overbevise os om at spise mere klimavenligt</a>, POV International (27/04/2023)</li>
                  </ul></li>
                  <li><a href="#c3nlp_values">Cultural Bias in ChatGPT:</a>
                  <ul>
                    <li><a href="https://politiken.dk/debat/klummer/jarlner/art9429359/Samtalerobot-er-et-redskab-for-amerikansk-kulturimperialisme">Samtalerobot er et redskab for amerikansk kulturimperialisme</a>, Politiken (10/07/2023)</li>
                    <li><a href="https://ekstrabladet.dk/nyheder/samfund/chatgpt-fremmer-amerikanske-normer-og-vaerdier/9856186">ChatGPT fremmer amerikanske normer og værdier</a>, Ekstra Bladet (10/07/2023)</li>
                    <li><a href="https://www.dr.dk/lyd/p1/p1-morgen/p1-morgen-2023-07-10">Studie: ChatGPT udbreder amerikanske normer</a>, P1 Morgen (10/07/2023)</li>
                    <li><a href="https://www.tv2kosmopol.dk/nyhedsarkiv?date=2023-07-10&clip=634dda2b-8303-4527-aeff-a96418116135">ChatGPT har amerikansk bias</a>, TV 2 Kosmopol (10/07/2023)</li>
                    <li><a href="https://borsen.dk/nyheder/ai/populaer-chatbot-promoverer-amerikanske-vaerdier-og-normer">Nyt studie: Populær chatbot promoverer amerikanske værdier og normer</a>, Børsen (25/08/2023)</li>
                  </ul></li>
                  <li><a href="https://gradynewsource.uga.edu/danish-researchers-look-to-ai-for-sustainable-future/">Danish Researchers Look To AI For Sustainable Future</a>, Grady Newsource, Georgia, USA (21/07/2023)</li>
                  <li><a href="https://techmanagement.dk/holdning/adjunkt-i-sprogmodeller-ledere-skal-mindske-risici-bias-i-generativ-ai">Adjunkt i sprogmodeller: Ledere skal mindske risici for bias i "generativ AI"</a>, Tech Management, Teknologiens Mediehus (1/09/2023)</li>
                </ul>
              </div>
            </div>

            <div class="card">
              <h2 class="header card-header" style="display: inline">
                <button id="misc" class="btn btn-link" data-toggle="collapse" data-target="#miscList" aria-expanded="true" aria-controls="miscList">Personal</button>
              </h2>
              <div id="miscList" class="misc collapse show" data-parent="#accordion">
                <ul class="card-body">
                  <li>I have been living in Denmark with my wonderful family since 2019.</li>
                  <li>I have been vegan for moral reasons since I was 15.</li>
                  <li>I used (and hope to continue) to practice Kendo (Japanese fencing). I was even at the World Championships in
                    <a href="#" onclick="return showImage('wkc16');">Japan (2015)</a> and
                    <a href="#" onclick="return showImage('wkc17');">Korea (2018)</a>.
                    <div id="wkc16" style="display: none"><img src="" style="width: 60%" alt="WKC16"/></div>
                    <div id="wkc17" style="display: none"><img src="" style="width: 60%" alt="WKC17"/></div>
                  </li>
                </ul>
              </div>
            </div>

            <div class="card">
              <h2 class="header card-header" style="display: inline">
                <button id="contact_info" class="btn btn-link" data-toggle="collapse" data-target="#contactList" aria-expanded="true" aria-controls="contactList">Contact</button>
              </h2>
              <div id="contactList" class="contact collapse show" data-parent="#accordion">
                <p class="card-body">
                Daniel Hershcovich<br>
                <a href="https://di.ku.dk/english" target="_blank">Department of Computer Science</a><br>
                <a href="https://www.ku.dk/english" target="_blank">University of Copenhagen</a> <br>
                Lyngbyvej 2<br>
                DK-2100 Copenhagen Ø, Denmark<br>
                <a id="email" onclick="grecaptcha.execute();" class="g-recaptcha"
                                                              data-sitekey="6Lc3G5UUAAAAAIGbANLYoplrZU9eviZEMrfJsKQ5" data-callback="showEmail">
                  d...
                </a>@di.ku.dk
                </p>
              </div>
            </div>

        </div>
    </div>
</div>

<a class="github-button" href="https://github.com/danielhers/danielhers.github.io" data-icon="octicon-repo-template"
                                                                                   aria-label="Use this template danielhers/danielhers.github.io on GitHub">Use this template</a>

<script src="https://code.jquery.com/jquery-3.2.1.min.js"></script>
<script type="text/javascript" src="js/jquery.ajax-cross-origin.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/popper.js/1.11.0/umd/popper.min.js"
        integrity="sha384-b/U6ypiBEHpOf/4+1nzFpr53nxSS+GLCkfwBdFNTxtclqqenISfwAzpKaMNFNmj4"
        crossorigin="anonymous"></script>
<script src="https://maxcdn.bootstrapcdn.com/bootstrap/4.0.0-beta/js/bootstrap.min.js"
        integrity="sha384-h0AbiXch4ZDo7tp9hKZ4TsHbi047NrKGLO3SEJAg45jXxnGIfYzk4Si90RDIqNm1"
        crossorigin="anonymous"></script>
<script src='https://www.google.com/recaptcha/api.js' async defer></script>
<script>
  $(function () {
      $(".menu").load("menu.html");
    });

  function showEmail() {
      $("#email").html("dh");
    }

  function showImage(name) {
      var i = $('#' + name);
      i.slideToggle();
      i.find('img').attr('src', 'index_files/' + name + '.jpg');
      return false;
    }

  function embedBibtex() {
      $(".bib").click(function (e) {
          e.preventDefault();
          let button = $(this);
          let container = button.parent().parent().parent();
          let bib_id = container.find("a").attr("id") + "_bib";
          let bibs = $("#" + bib_id);
          button.toggleClass("active");
          if (bibs.length) {
              bibs.slideToggle();
            } else {
                $.ajax({
                    url: button.attr("href"), dataType: "text", success: function (data) {
                        container.append("<div id='" + bib_id + "' class='bibtex row' style='display:none;'><pre>" + data + "</pre></div>");
                          $("#" + bib_id).slideToggle();
                          }
                          });
                          }
                          })
                          }

                          window.onload = embedBibtex;

                          // For Google Analytics
                          window.dataLayer = window.dataLayer || [];

                          function gtag() {
                          dataLayer.push(arguments);
                          }

                          gtag('js', new Date());

                          gtag('config', 'UA-101317305-2');
</script>

</body>

</html>
