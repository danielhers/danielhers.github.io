@inproceedings{bollmann-etal-2021-moses,
    title = "{M}oses and the Character-Based Random Babbling Baseline: {C}o{AS}ta{L} at {A}mericas{NLP} 2021 Shared Task",
    author = "Bollmann, Marcel  and
      Aralikatte, Rahul  and
      Murrieta Bello, H{\'e}ctor  and
      Hershcovich, Daniel  and
      de Lhoneux, Miryam  and
      S{\o}gaard, Anders",
    booktitle = "Proceedings of the First Workshop on Natural Language Processing for Indigenous Languages of the Americas",
    month = jun,
    year = "2021",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2021.americasnlp-1.28",
    doi = "10.18653/v1/2021.americasnlp-1.28",
    pages = "248--254",
    abstract = "We evaluated a range of neural machine translation techniques developed specifically for low-resource scenarios. Unsuccessfully. In the end, we submitted two runs: (i) a standard phrase-based model, and (ii) a random babbling baseline using character trigrams. We found that it was surprisingly hard to beat (i), in spite of this model being, in theory, a bad fit for polysynthetic languages; and more interestingly, that (ii) was better than several of the submitted systems, highlighting how difficult low-resource machine translation for polysynthetic languages is.",
}
