@inproceedings{karamolegkou-etal-2024-vision,
    title = "Vision-Language Models under Cultural and Inclusive Considerations",
    author = "Karamolegkou, Antonia  and
      Rust, Phillip  and
      Cui, Ruixiang  and
      Cao, Yong  and
      S{\o}gaard, Anders  and
      Hershcovich, Daniel",
    editor = "Soni, Nikita  and
      Flek, Lucie  and
      Sharma, Ashish  and
      Yang, Diyi  and
      Hooker, Sara  and
      Schwartz, H. Andrew",
    booktitle = "Proceedings of the 1st Human-Centered Large Language Modeling Workshop",
    month = aug,
    year = "2024",
    address = "TBD",
    publisher = "ACL",
    url = "https://aclanthology.org/2024.hucllm-1.5",
    pages = "53--66",
    abstract = "Large Vision Language Models can be used to assist visually impaired individuals by describing images they capture in their daily lives. Current evaluation datasets may not reflect the diverse cultural user backgrounds nor the situational context of this use case. To address this problem, we create a survey to determine caption preferences and propose a culture-centric evaluation benchmark by filtering VizWiz, an existing dataset with images taken by people who are blind. We then evaluate different models and prompts, investigating their reliability as visual assistants. While the evaluation results for state-of-the-art models seem promising, we identified some weak spots such as hallucinations and problems with conventional evaluation metrics. Our survey, data, code, and model outputs will be publicly available.",
}
